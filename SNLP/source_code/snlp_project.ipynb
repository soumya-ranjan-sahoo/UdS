{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNLP 2019 Final Project:\n",
    "\n",
    "## Case-study of Offensive Language Classification from Online Tweets\n",
    "\n",
    "**Team members**\n",
    "\n",
    "|Name|Matriculation Number|Email address|\n",
    "|----|--------------------|-------------|\n",
    "|Jyotsna Singh|2576744|s8jysing@stud.uni-saarland.de|\n",
    "|Soumya Ranjan Sahoo|2576610|s8sosaho@stud.uni-saarland.de|\n",
    "|Sourav Dutta|2576494|s8sodutt@stud.uni-saarland.de|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Multinomial Naive Bayes classification\n",
    "\n",
    "### Load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import subprocess as sp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file_path):\n",
    "    return pd.read_csv(data_file_path, sep='\\t', names=['Sentiment', 'Text'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data files\n",
    "\n",
    "train_file = 'offensive_dataset/train.tsv'\n",
    "dev_file   = 'offensive_dataset/dev.tsv'\n",
    "\n",
    "train_data = load_data(train_file)\n",
    "dev_data   = load_data(dev_file)\n",
    "\n",
    "train_data['y'] = np.where(train_data['Sentiment'] == 'OFF', 1, 0)\n",
    "dev_data['y'] = np.where(dev_data['Sentiment'] == 'OFF', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "  Sentiment                                               Text  y\n",
      "0       NOT                             @USER @USER Yeah he is  0\n",
      "1       OFF  @USER @USER @USER I'm assuming that @USER is a...  1\n",
      "2       NOT     @USER you are not the true Columbia Bugle. Gfy  0\n",
      "3       NOT  @USER @USER There ain't no MAGA hats sold here...  0\n",
      "4       OFF                              @USER Lmao fuck u bae  1\n",
      "\n",
      "Dev data:\n",
      "  Sentiment                                               Text  y\n",
      "0       NOT  @USER He never was one much for the rule of la...  0\n",
      "1       OFF  @USER I think Donald Trump is a disgusting dis...  1\n",
      "2       NOT                           @USER Um bc she is??????  0\n",
      "3       OFF  @USER @USER @USER @USER @USER @USER @USER @USE...  1\n",
      "4       NOT  @USER It would be so nice if the Trump support...  0\n"
     ]
    }
   ],
   "source": [
    "print('Train data:\\n{}'.format(train_data.head()))\n",
    "print('\\nDev data:\\n{}'.format(dev_data.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted to file \"train_text.txt\".\n",
      "Text extracted to file \"dev_text.txt\".\n"
     ]
    }
   ],
   "source": [
    "# extract tweet text into a separate file for tokenization\n",
    "\n",
    "def extract_data_to_file(data, file_name):\n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        for text in data['Text']:\n",
    "            file.write(text + '\\n')\n",
    "    print('Text extracted to file \"{}\".'.format(file_name))\n",
    "\n",
    "extract_data_to_file(train_data, 'train_text.txt')\n",
    "extract_data_to_file(dev_data, 'dev_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove local file\n",
    "\n",
    "def remove_file(file):\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "    else:\n",
    "        print(\"File '{}' does not exist! Try again.\".format(file))\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra step of tokenizing multiple emojis into individual emoji characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize multi-emojis into separate emojis\n",
    "\n",
    "def tokenize_emojis(text):\n",
    "    \n",
    "    RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "    \n",
    "    result = ''\n",
    "    for word in text.split():\n",
    "        for char in word:\n",
    "            if re.match(RE_EMOJI, char):\n",
    "                result += ' ' + char + ' '\n",
    "            else:\n",
    "                result += char\n",
    "        result += ' '\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "@USER @USER @USER Everyone muted you ...  Thanks for working for free 😂😂😂  #MAGA\n",
      "@USER @USER @USER Everyone muted you ... Thanks for working for free  😂  😂  😂  #MAGA \n",
      "\n",
      ".@USER Today I can announce that new longer-term partnerships will be opened up to the most ambitious housing associations through a ground-breaking £2 billion initiative...the first time any government has offered housing associations such long-term certainty\"  🏘🏘🏘🏘 URL\n",
      ".@USER Today I can announce that new longer-term partnerships will be opened up to the most ambitious housing associations through a ground-breaking £2 billion initiative...the first time any government has offered housing associations such long-term certainty\"  🏘  🏘  🏘  🏘  URL \n",
      "\n",
      "6Lack just made it on my sex playlist😭😭🔥🔥 baby wam wherever you are get ready to get pregnant cause wow😭😭🔥\n",
      "6Lack just made it on my sex playlist 😭  😭  🔥  🔥  baby wam wherever you are get ready to get pregnant cause wow 😭  😭  🔥  \n",
      "\n",
      "@USER @USER @USER @USER @USER @USER @USER @USER @USER @USER You are so beautiful !!😢😍Xx \n",
      "@USER @USER @USER @USER @USER @USER @USER @USER @USER @USER You are so beautiful !! 😢  😍 Xx \n"
     ]
    }
   ],
   "source": [
    "# testing the tokenization process of multi-emojis\n",
    "\n",
    "texts = [\n",
    "    '@USER @USER @USER Everyone muted you ...  Thanks for working for free 😂😂😂  #MAGA',\n",
    "    '.@USER Today I can announce that new longer-term partnerships will be opened up to the most ambitious housing associations through a ground-breaking £2 billion initiative...the first time any government has offered housing associations such long-term certainty\"  🏘🏘🏘🏘 URL',\n",
    "    '6Lack just made it on my sex playlist😭😭🔥🔥 baby wam wherever you are get ready to get pregnant cause wow😭😭🔥',\n",
    "    '@USER @USER @USER @USER @USER @USER @USER @USER @USER @USER You are so beautiful !!😢😍Xx '\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print()\n",
    "    print(text)\n",
    "    print(tokenize_emojis(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization of file \"train_text.txt\" done.\n",
      "\n",
      "Tokenization of file \"dev_text.txt\" done.\n"
     ]
    }
   ],
   "source": [
    "# takes around 5 seconds to run\n",
    "\n",
    "def tokenize(file, data):\n",
    "    process = sp.Popen(['sh', 'twokenize.sh', file], stdout=sp.PIPE)\n",
    "    with open('tokenized.txt', 'wb') as write_file:\n",
    "        for line in process.stdout.readlines():\n",
    "            write_file.write(line)\n",
    "    tokenized = []\n",
    "    with open('tokenized.txt', 'r', encoding='utf-8') as read_file:\n",
    "        for line in read_file:\n",
    "            tokenized.append(tokenize_emojis(line.split('\\t')[0]))\n",
    "    print('\\nTokenization of file \"{}\" done.'.format(file))\n",
    "    return tokenized\n",
    "\n",
    "tokenized_train_data = tokenize('train_text.txt', train_data)\n",
    "tokenized_dev_data = tokenize('dev_text.txt', dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training data: \t 11240\n",
      "Number of samples in dev data: \t\t 1000\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples in training data: \\t', len(tokenized_train_data))\n",
    "print('Number of samples in dev data: \\t\\t', len(tokenized_dev_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing supervised classifier model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels in Dev data:  1000\n"
     ]
    }
   ],
   "source": [
    "#labels NOT-> 0, OFF -> 1\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "train_labels =  train_data['Sentiment']\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels = le.transform(train_labels)\n",
    "type(train_labels)\n",
    "\n",
    "\n",
    "dev_labels =  dev_data['Sentiment']\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(dev_labels)\n",
    "dev_labels = le.transform(dev_labels)\n",
    "print('Number of labels in Dev data: ', len(dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     \n",
    "                     ('clf', MultinomialNB())])\n",
    "\n",
    "tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    \n",
    "    'clf__alpha': [1, 1e-1, 1e-2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for f1_macro\n",
      "\n",
      "Best parameters set found on training set:\n",
      "\n",
      "{'clf__alpha': 0.1, 'vect__ngram_range': (1, 1)}\n",
      "\n",
      "Grid scores on training set:\n",
      "\n",
      "0.659 (+/-0.007) for {'clf__alpha': 1, 'vect__ngram_range': (1, 1)}\n",
      "0.561 (+/-0.015) for {'clf__alpha': 1, 'vect__ngram_range': (1, 2)}\n",
      "0.542 (+/-0.011) for {'clf__alpha': 1, 'vect__ngram_range': (2, 2)}\n",
      "0.665 (+/-0.013) for {'clf__alpha': 0.1, 'vect__ngram_range': (1, 1)}\n",
      "0.644 (+/-0.008) for {'clf__alpha': 0.1, 'vect__ngram_range': (1, 2)}\n",
      "0.593 (+/-0.008) for {'clf__alpha': 0.1, 'vect__ngram_range': (2, 2)}\n",
      "0.652 (+/-0.011) for {'clf__alpha': 0.01, 'vect__ngram_range': (1, 1)}\n",
      "0.626 (+/-0.006) for {'clf__alpha': 0.01, 'vect__ngram_range': (1, 2)}\n",
      "0.591 (+/-0.009) for {'clf__alpha': 0.01, 'vect__ngram_range': (2, 2)}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full training set.\n",
      "The scores are computed on the full development set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.6418    0.7920    0.7090       500\n",
      "          1     0.7285    0.5580    0.6319       500\n",
      "\n",
      "avg / total     0.6851    0.6750    0.6705      1000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes  (without tf-idf) (takes less than 1 minute to run)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "score = 'f1_macro'\n",
    "print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "print()\n",
    "np.errstate(divide='ignore')\n",
    "clf = GridSearchCV(text_clf, tuned_parameters, cv=3, scoring=score)\n",
    "clf.fit(tokenized_train_data, train_labels)\n",
    "\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on training set:\")\n",
    "print()\n",
    "for mean, std, params in zip(clf.cv_results_['mean_test_score'], \n",
    "                             clf.cv_results_['std_test_score'], \n",
    "                             clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full training set.\")\n",
    "print(\"The scores are computed on the full development set.\")\n",
    "print()\n",
    "print(classification_report(dev_labels, clf.predict(tokenized_dev_data), digits=4))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some sample wrong classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing   6 wrong predictions :\n",
      "\n",
      "@USER @USER There ain't no MAGA hats sold here but that don't stop me from wanting to MAGA.\n",
      "\n",
      "\n",
      "@USER Lmao fuck u bae\n",
      "\n",
      "\n",
      "@USER But I thought Antifa were actually fascists but this shows Rethuglicans are the actual fascists in the room.\n",
      "\n",
      "\n",
      "@USER @USER @USER Where is VAN? She is ARMY who need to shazam the most you know... 🔍🐾🐶🐾🔎\n",
      "\n",
      "\n",
      "#LeviStrauss Takes Stand On #GunControl #CBS #SanFrancisco URL #NoMoreLevis #guns #NRA #USA @USER #GOP #libertarian #conservative\n",
      "\n",
      "\n",
      "@USER @USER Right?! I wanna know why she on the phone calling the police anyway? what did he do\"? Fuckin \"Run Me Over Rhonda\" lol\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#wrong predictions\n",
    "\n",
    "list_diff = dev_labels - clf.predict(tokenized_dev_data)\n",
    "list_diff = list(list_diff)\n",
    "list_1 = [i for i, e in enumerate(list_diff) if e == 1] #actual 1(off)\n",
    "list_0 = [i for i, e in enumerate(list_diff) if e == -1] #actual 0(not)\n",
    "    \n",
    "# 6 wrong predictions\n",
    "def no_of_wrong_preds(num):\n",
    "    print(\"Printing % 3d wrong predictions :\\n\" %(num))\n",
    "    for i in range (int(num/2)):\n",
    "        print(train_data['Text'][list_1[i]])\n",
    "        print()\n",
    "        print()\n",
    "        print(train_data['Text'][list_0[i]])\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "no_of_wrong_preds(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv': 3,\n",
       " 'error_score': 'raise',\n",
       " 'estimator__memory': None,\n",
       " 'estimator__steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)),\n",
       "  ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       " 'estimator__vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'estimator__clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " 'estimator__vect__analyzer': 'word',\n",
       " 'estimator__vect__binary': False,\n",
       " 'estimator__vect__decode_error': 'strict',\n",
       " 'estimator__vect__dtype': numpy.int64,\n",
       " 'estimator__vect__encoding': 'utf-8',\n",
       " 'estimator__vect__input': 'content',\n",
       " 'estimator__vect__lowercase': True,\n",
       " 'estimator__vect__max_df': 1.0,\n",
       " 'estimator__vect__max_features': None,\n",
       " 'estimator__vect__min_df': 1,\n",
       " 'estimator__vect__ngram_range': (1, 1),\n",
       " 'estimator__vect__preprocessor': None,\n",
       " 'estimator__vect__stop_words': None,\n",
       " 'estimator__vect__strip_accents': None,\n",
       " 'estimator__vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'estimator__vect__tokenizer': None,\n",
       " 'estimator__vect__vocabulary': None,\n",
       " 'estimator__clf__alpha': 1.0,\n",
       " 'estimator__clf__class_prior': None,\n",
       " 'estimator__clf__fit_prior': True,\n",
       " 'estimator': Pipeline(memory=None,\n",
       "      steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " 'fit_params': None,\n",
       " 'iid': True,\n",
       " 'n_jobs': 1,\n",
       " 'param_grid': {'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
       "  'clf__alpha': [1, 0.1, 0.01]},\n",
       " 'pre_dispatch': '2*n_jobs',\n",
       " 'refit': True,\n",
       " 'return_train_score': 'warn',\n",
       " 'scoring': 'f1_macro',\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test predictions using Method 1 (Multinomial Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "\n",
    "test_file  = 'offensive_dataset/test.tsv'\n",
    "test_data = pd.read_csv(test_file, names=['Text'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@USER This stinks like week old fish she is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USER @USER Wrong 60% didn't vote PC so that d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@USER Goodness. Your wife and ex-gf were both ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@USER @USER And yet he is allowed to. Congress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USER @USER #Westminster @USER #Tories @USER @...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  @USER This stinks like week old fish she is a ...\n",
       "1  @USER @USER Wrong 60% didn't vote PC so that d...\n",
       "2  @USER Goodness. Your wife and ex-gf were both ...\n",
       "3  @USER @USER And yet he is allowed to. Congress...\n",
       "4  @USER @USER #Westminster @USER #Tories @USER @..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted to file \"test_text.txt\".\n",
      "\n",
      "Tokenization of file \"test_text.txt\" done.\n"
     ]
    }
   ],
   "source": [
    "# extract tests data to file\n",
    "\n",
    "extract_data_to_file(test_data, 'test_text.txt')\n",
    "\n",
    "# tokenize the test data\n",
    "\n",
    "tokenized_test_data = tokenize('test_text.txt', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@USER @USER Wrong 60% didn't vote PC so that doesn't mean the 60% who didn't should shut up . Likewise a 1/3 voted NDP so not a majority by any means but a sizeable minority . Actually Liberals and Greens have similar position too and vote wise those three are majority . \n",
      "\n",
      "Number of test data samples:  1000\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_test_data[1])\n",
    "print()\n",
    "print('Number of test data samples: ', len(tokenized_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = clf.predict(tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total labels: 1000\n",
      "Total class \"0\" labels: 627\n",
      "Total class \"1\" labels: 373\n"
     ]
    }
   ],
   "source": [
    "print('Total labels: {}'.format(len(test_predictions)))\n",
    "print('Total class \"0\" labels: {}'.format(len(test_predictions) - sum(test_predictions)))\n",
    "print('Total class \"1\" labels: {}'.format(sum(test_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions written to file.\n"
     ]
    }
   ],
   "source": [
    "with open('predictions.test', 'w', encoding='utf-8') as test_pred_file:\n",
    "    for pred in test_predictions:\n",
    "        test_pred_file.write('OFF\\n' if pred == 1 else 'NOT\\n')\n",
    "    print('Test predictions written to file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Multinomial Naive Bayes + _tf-idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "\n",
    "tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': [1, 1e-1, 1e-2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for f1_macro\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sourav/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/sourav/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/sourav/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/sourav/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on training set:\n",
      "\n",
      "{'clf__alpha': 0.01, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
      "\n",
      "Grid scores on training set:\n",
      "\n",
      "0.418 (+/-0.003) for {'clf__alpha': 1, 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
      "0.412 (+/-0.001) for {'clf__alpha': 1, 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
      "0.417 (+/-0.000) for {'clf__alpha': 1, 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (2, 2)}\n",
      "0.411 (+/-0.001) for {'clf__alpha': 1, 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n",
      "0.411 (+/-0.000) for {'clf__alpha': 1, 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}\n",
      "0.417 (+/-0.001) for {'clf__alpha': 1, 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (2, 2)}\n",
      "0.469 (+/-0.008) for {'clf__alpha': 1, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
      "0.433 (+/-0.002) for {'clf__alpha': 1, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
      "0.430 (+/-0.001) for {'clf__alpha': 1, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (2, 2)}\n",
      "0.429 (+/-0.004) for {'clf__alpha': 1, 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n",
      "0.421 (+/-0.002) for {'clf__alpha': 1, 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}\n",
      "0.428 (+/-0.001) for {'clf__alpha': 1, 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (2, 2)}\n",
      "0.453 (+/-0.000) for {'clf__alpha': 0.1, 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
      "0.428 (+/-0.002) for {'clf__alpha': 0.1, 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
      "0.435 (+/-0.003) for {'clf__alpha': 0.1, 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (2, 2)}\n",
      "0.425 (+/-0.000) for {'clf__alpha': 0.1, 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n",
      "0.420 (+/-0.001) for {'clf__alpha': 0.1, 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}\n",
      "0.430 (+/-0.003) for {'clf__alpha': 0.1, 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (2, 2)}\n",
      "0.615 (+/-0.013) for {'clf__alpha': 0.1, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
      "0.561 (+/-0.018) for {'clf__alpha': 0.1, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
      "0.547 (+/-0.013) for {'clf__alpha': 0.1, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (2, 2)}\n",
      "0.546 (+/-0.021) for {'clf__alpha': 0.1, 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n",
      "0.503 (+/-0.020) for {'clf__alpha': 0.1, 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}\n",
      "0.519 (+/-0.010) for {'clf__alpha': 0.1, 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (2, 2)}\n",
      "0.492 (+/-0.011) for {'clf__alpha': 0.01, 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
      "0.476 (+/-0.006) for {'clf__alpha': 0.01, 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
      "0.508 (+/-0.010) for {'clf__alpha': 0.01, 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (2, 2)}\n",
      "0.446 (+/-0.015) for {'clf__alpha': 0.01, 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n",
      "0.443 (+/-0.001) for {'clf__alpha': 0.01, 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}\n",
      "0.487 (+/-0.011) for {'clf__alpha': 0.01, 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (2, 2)}\n",
      "0.623 (+/-0.010) for {'clf__alpha': 0.01, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
      "0.614 (+/-0.002) for {'clf__alpha': 0.01, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
      "0.576 (+/-0.008) for {'clf__alpha': 0.01, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (2, 2)}\n",
      "0.596 (+/-0.020) for {'clf__alpha': 0.01, 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n",
      "0.600 (+/-0.010) for {'clf__alpha': 0.01, 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}\n",
      "0.570 (+/-0.004) for {'clf__alpha': 0.01, 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (2, 2)}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full training set.\n",
      "The scores are computed on the full development set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.5647    0.8640    0.6830       500\n",
      "          1     0.7106    0.3340    0.4544       500\n",
      "\n",
      "avg / total     0.6377    0.5990    0.5687      1000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with tf-idf (takes less than 1 minute to run)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "score = 'f1_macro'\n",
    "print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "print()\n",
    "np.errstate(divide='ignore')\n",
    "clf = GridSearchCV(text_clf, tuned_parameters, cv=2, scoring=score)\n",
    "clf.fit(tokenized_train_data, train_labels)\n",
    "\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on training set:\")\n",
    "print()\n",
    "for mean, std, params in zip(clf.cv_results_['mean_test_score'], \n",
    "                             clf.cv_results_['std_test_score'], \n",
    "                             clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full training set.\")\n",
    "print(\"The scores are computed on the full development set.\")\n",
    "print()\n",
    "print(classification_report(dev_labels, clf.predict(tokenized_dev_data), digits=4))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Advanced Analysis: FastText _(Joulin et. al.)_\n",
    "\n",
    "**NOTE:** This FastText implementation is carried out with minimal values of its hyperparameters (learning rate: 0.1, epochs: 20, dimensions: 20, character ngrams: bigrams). Using higher values of certain hyperparameters may increase its overall accuracy values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import emoji\n",
    "\n",
    "import nltk\n",
    "import csv\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESSING\n",
    "\n",
    "\n",
    "def load_dict_smileys():\n",
    "    \n",
    "    return {\n",
    "        \":‑)\":\"smiley\",\n",
    "        \":-]\":\"smiley\",\n",
    "        \":-3\":\"smiley\",\n",
    "        \":->\":\"smiley\",\n",
    "        \"8-)\":\"smiley\",\n",
    "        \":-}\":\"smiley\",\n",
    "        \":)\":\"smiley\",\n",
    "        \":]\":\"smiley\",\n",
    "        \":3\":\"smiley\",\n",
    "        \":>\":\"smiley\",\n",
    "        \"8)\":\"smiley\",\n",
    "        \":}\":\"smiley\",\n",
    "        \":o)\":\"smiley\",\n",
    "        \":c)\":\"smiley\",\n",
    "        \":^)\":\"smiley\",\n",
    "        \"=]\":\"smiley\",\n",
    "        \"=)\":\"smiley\",\n",
    "        \":-))\":\"smiley\",\n",
    "        \":‑D\":\"smiley\",\n",
    "        \"8‑D\":\"smiley\",\n",
    "        \"x‑D\":\"smiley\",\n",
    "        \"X‑D\":\"smiley\",\n",
    "        \":D\":\"smiley\",\n",
    "        \"8D\":\"smiley\",\n",
    "        \"xD\":\"smiley\",\n",
    "        \"XD\":\"smiley\",\n",
    "        \":‑(\":\"sad\",\n",
    "        \":‑c\":\"sad\",\n",
    "        \":‑<\":\"sad\",\n",
    "        \":‑[\":\"sad\",\n",
    "        \":(\":\"sad\",\n",
    "        \":c\":\"sad\",\n",
    "        \":<\":\"sad\",\n",
    "        \":[\":\"sad\",\n",
    "        \":-||\":\"sad\",\n",
    "        \">:[\":\"sad\",\n",
    "        \":{\":\"sad\",\n",
    "        \":@\":\"sad\",\n",
    "        \">:(\":\"sad\",\n",
    "        \":'‑(\":\"sad\",\n",
    "        \":'(\":\"sad\",\n",
    "        \":‑P\":\"playful\",\n",
    "        \"X‑P\":\"playful\",\n",
    "        \"x‑p\":\"playful\",\n",
    "        \":‑p\":\"playful\",\n",
    "        \":‑Þ\":\"playful\",\n",
    "        \":‑þ\":\"playful\",\n",
    "        \":‑b\":\"playful\",\n",
    "        \":P\":\"playful\",\n",
    "        \"XP\":\"playful\",\n",
    "        \"xp\":\"playful\",\n",
    "        \":p\":\"playful\",\n",
    "        \":Þ\":\"playful\",\n",
    "        \":þ\":\"playful\",\n",
    "        \":b\":\"playful\",\n",
    "        \"<3\":\"love\"\n",
    "        }\n",
    "\n",
    "\n",
    "def load_dict_contractions():\n",
    "    \n",
    "    return {\n",
    "        \"ain't\":\"is not\",\n",
    "        \"amn't\":\"am not\",\n",
    "        \"aren't\":\"are not\",\n",
    "        \"can't\":\"cannot\",\n",
    "        \"'cause\":\"because\",\n",
    "        \"couldn't\":\"could not\",\n",
    "        \"couldn't've\":\"could not have\",\n",
    "        \"could've\":\"could have\",\n",
    "        \"daren't\":\"dare not\",\n",
    "        \"daresn't\":\"dare not\",\n",
    "        \"dasn't\":\"dare not\",\n",
    "        \"didn't\":\"did not\",\n",
    "        \"doesn't\":\"does not\",\n",
    "        \"don't\":\"do not\",\n",
    "        \"e'er\":\"ever\",\n",
    "        \"em\":\"them\",\n",
    "        \"everyone's\":\"everyone is\",\n",
    "        \"finna\":\"fixing to\",\n",
    "        \"gimme\":\"give me\",\n",
    "        \"gonna\":\"going to\",\n",
    "        \"gon't\":\"go not\",\n",
    "        \"gotta\":\"got to\",\n",
    "        \"hadn't\":\"had not\",\n",
    "        \"hasn't\":\"has not\",\n",
    "        \"haven't\":\"have not\",\n",
    "        \"he'd\":\"he would\",\n",
    "        \"he'll\":\"he will\",\n",
    "        \"he's\":\"he is\",\n",
    "        \"he've\":\"he have\",\n",
    "        \"how'd\":\"how would\",\n",
    "        \"how'll\":\"how will\",\n",
    "        \"how're\":\"how are\",\n",
    "        \"how's\":\"how is\",\n",
    "        \"I'd\":\"I would\",\n",
    "        \"I'll\":\"I will\",\n",
    "        \"I'm\":\"I am\",\n",
    "        \"I'm'a\":\"I am about to\",\n",
    "        \"I'm'o\":\"I am going to\",\n",
    "        \"isn't\":\"is not\",\n",
    "        \"it'd\":\"it would\",\n",
    "        \"it'll\":\"it will\",\n",
    "        \"it's\":\"it is\",\n",
    "        \"I've\":\"I have\",\n",
    "        \"kinda\":\"kind of\",\n",
    "        \"let's\":\"let us\",\n",
    "        \"mayn't\":\"may not\",\n",
    "        \"may've\":\"may have\",\n",
    "        \"mightn't\":\"might not\",\n",
    "        \"might've\":\"might have\",\n",
    "        \"mustn't\":\"must not\",\n",
    "        \"mustn't've\":\"must not have\",\n",
    "        \"must've\":\"must have\",\n",
    "        \"needn't\":\"need not\",\n",
    "        \"ne'er\":\"never\",\n",
    "        \"o'\":\"of\",\n",
    "        \"o'er\":\"over\",\n",
    "        \"ol'\":\"old\",\n",
    "        \"oughtn't\":\"ought not\",\n",
    "        \"shalln't\":\"shall not\",\n",
    "        \"shan't\":\"shall not\",\n",
    "        \"she'd\":\"she would\",\n",
    "        \"she'll\":\"she will\",\n",
    "        \"she's\":\"she is\",\n",
    "        \"shouldn't\":\"should not\",\n",
    "        \"shouldn't've\":\"should not have\",\n",
    "        \"should've\":\"should have\",\n",
    "        \"somebody's\":\"somebody is\",\n",
    "        \"someone's\":\"someone is\",\n",
    "        \"something's\":\"something is\",\n",
    "        \"that'd\":\"that would\",\n",
    "        \"that'll\":\"that will\",\n",
    "        \"that're\":\"that are\",\n",
    "        \"that's\":\"that is\",\n",
    "        \"there'd\":\"there would\",\n",
    "        \"there'll\":\"there will\",\n",
    "        \"there're\":\"there are\",\n",
    "        \"there's\":\"there is\",\n",
    "        \"these're\":\"these are\",\n",
    "        \"they'd\":\"they would\",\n",
    "        \"they'll\":\"they will\",\n",
    "        \"they're\":\"they are\",\n",
    "        \"they've\":\"they have\",\n",
    "        \"this's\":\"this is\",\n",
    "        \"those're\":\"those are\",\n",
    "        \"'tis\":\"it is\",\n",
    "        \"'twas\":\"it was\",\n",
    "        \"wanna\":\"want to\",\n",
    "        \"wasn't\":\"was not\",\n",
    "        \"we'd\":\"we would\",\n",
    "        \"we'd've\":\"we would have\",\n",
    "        \"we'll\":\"we will\",\n",
    "        \"we're\":\"we are\",\n",
    "        \"weren't\":\"were not\",\n",
    "        \"we've\":\"we have\",\n",
    "        \"what'd\":\"what did\",\n",
    "        \"what'll\":\"what will\",\n",
    "        \"what're\":\"what are\",\n",
    "        \"what's\":\"what is\",\n",
    "        \"what've\":\"what have\",\n",
    "        \"when's\":\"when is\",\n",
    "        \"where'd\":\"where did\",\n",
    "        \"where're\":\"where are\",\n",
    "        \"where's\":\"where is\",\n",
    "        \"where've\":\"where have\",\n",
    "        \"which's\":\"which is\",\n",
    "        \"who'd\":\"who would\",\n",
    "        \"who'd've\":\"who would have\",\n",
    "        \"who'll\":\"who will\",\n",
    "        \"who're\":\"who are\",\n",
    "        \"who's\":\"who is\",\n",
    "        \"who've\":\"who have\",\n",
    "        \"why'd\":\"why did\",\n",
    "        \"why're\":\"why are\",\n",
    "        \"why's\":\"why is\",\n",
    "        \"won't\":\"will not\",\n",
    "        \"wouldn't\":\"would not\",\n",
    "        \"would've\":\"would have\",\n",
    "        \"y'all\":\"you all\",\n",
    "        \"you'd\":\"you would\",\n",
    "        \"you'll\":\"you will\",\n",
    "        \"you're\":\"you are\",\n",
    "        \"you've\":\"you have\",\n",
    "        \"Whatcha\":\"What are you\",\n",
    "        \"luv\":\"love\",\n",
    "        \"sux\":\"sucks\"\n",
    "        }\n",
    "\n",
    "\n",
    "def strip_accents(text):\n",
    "    if 'ø' in text or  'Ø' in text:\n",
    "        #Do nothing when finding ø \n",
    "        return text   \n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    text = text.decode(\"utf-8\")\n",
    "    return str(text)\n",
    "\n",
    "\n",
    "def tweet_cleaning_for_sentiment_analysis(tweet):    \n",
    "    \n",
    "    #Escaping HTML characters\n",
    "    tweet = BeautifulSoup(tweet).get_text()\n",
    "    #Special case not handled previously.\n",
    "    tweet = tweet.replace('\\x92',\"'\")\n",
    "    #Removal of hastags/account\n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", tweet).split())\n",
    "    #Removal of address\n",
    "    tweet = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    #Removal of Punctuation\n",
    "    tweet = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \", tweet).split())\n",
    "    #Lower case\n",
    "    tweet = tweet.lower()\n",
    "    #CONTRACTIONS source: https://en.wikipedia.org/wiki/Contraction_%28grammar%29\n",
    "    CONTRACTIONS = load_dict_contractions()\n",
    "    tweet = tweet.replace(\"’\",\"'\")\n",
    "    words = tweet.split()\n",
    "    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n",
    "    tweet = \" \".join(reformed)\n",
    "    # Standardizing words\n",
    "    tweet = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet))\n",
    "    #Deal with smileys\n",
    "    #source: https://en.wikipedia.org/wiki/List_of_emoticons\n",
    "    SMILEY = load_dict_smileys()  \n",
    "    words = tweet.split()\n",
    "    reformed = [SMILEY[word] if word in SMILEY else word for word in words]\n",
    "    tweet = \" \".join(reformed)\n",
    "    #Deal with emojis\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    #Strip accents\n",
    "    tweet= strip_accents(tweet)\n",
    "    tweet = tweet.replace(\":\",\" \")\n",
    "    tweet = ' '.join(tweet.split())\n",
    "    \n",
    "    # DO NOT REMOVE STOP WORDS FOR SENTIMENT ANALYSIS - OR AT LEAST NOT NEGATIVE ONES\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file_path):\n",
    "    return pd.read_csv(data_file_path, sep='\\t', names=['Sentiment', 'Text'], header=None)\n",
    "\n",
    "train_file = 'offensive_dataset/train.tsv'\n",
    "dev_file   = 'offensive_dataset/dev.tsv'\n",
    "\n",
    "train_data = load_data(train_file)\n",
    "dev_data   = load_data(dev_file)\n",
    "\n",
    "train_data['y'] = np.where(train_data['Sentiment'] == 'OFF', 1, 0)\n",
    "dev_data['y'] = np.where(dev_data['Sentiment'] == 'OFF', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_file='offensive_dataset/train.tsv'\n",
    "csv_table=pd.read_table(tsv_file,sep='\\t')\n",
    "csv_table.to_csv('train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_instance(row):\n",
    "    cur_row = []\n",
    "    #Prefix the index-ed label with __label__\n",
    "    label = \"__label__\" + row[1]['Sentiment']  \n",
    "    cur_row.append(label)\n",
    "    cur_row.extend(nltk.word_tokenize(tweet_cleaning_for_sentiment_analysis(row[1]['Text'].lower())))\n",
    "    return cur_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes around 10 seconds to run\n",
    "\n",
    "def preprocess(input_file, output_file, keep=1):\n",
    "    with open(output_file, 'w', encoding=\"utf8\") as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        for row in input_file.iterrows():\n",
    "            row_output = transform_instance(row)\n",
    "            csv_writer.writerow(row_output)\n",
    "\n",
    "preprocess(train_data, 'tweets.train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(dev_data, 'tweets.validate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '\\\\model\\\\'\n",
    "model_name = \"model-en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start\n",
      "\n",
      "2019-09-13 23:15:17.501386 START => {'lr': 0.01, 'epoch': 20, 'wordNgrams': 2, 'dim': 20}\n",
      "Model trained with the hyperparameter \n",
      " {'lr': 0.01, 'epoch': 20, 'wordNgrams': 2, 'dim': 20}\n",
      "2019-09-13 23:15:18.397257Training complete.{'lr': 0.01, 'epoch': 20, 'wordNgrams': 2, 'dim': 20}\n",
      "{'lr': 0.01, 'epoch': 20, 'wordNgrams': 2, 'dim': 20}\n",
      "\n",
      "Training Accuracy: 73.99466192170819 %\n",
      "Validation Accuracy: 55.50000000000001 %\n",
      "\n",
      "Testing\n",
      "\n",
      "\n",
      "Text: \"this player does not play well\"\n",
      "Predicted label:  NOT\n",
      "Confidence:  78.4713 %\n",
      "\n",
      "Text: \"this player is soooo shit\"\n",
      "Predicted label:  OFF\n",
      "Confidence:  94.1431 %\n"
     ]
    }
   ],
   "source": [
    "def print_test_prediction(model, sample_text):\n",
    "    pred = model.predict([sample_text], k=1)\n",
    "    print('\\nText: \"{}\"'.format(sample_text))\n",
    "    print('Predicted label: ', pred[0][0][0][-3:])\n",
    "    print('Confidence:  %.4f %%' % (pred[1][0][0]*100))\n",
    "\n",
    "    \n",
    "def train():\n",
    "    \n",
    "    print('Training start\\n')\n",
    "    \n",
    "    try:\n",
    "        hyper_params = {\"lr\": 0.01,\n",
    "                        \"epoch\": 20,\n",
    "                        \"wordNgrams\": 2,\n",
    "                        \"dim\": 20}     \n",
    "                               \n",
    "        print(str(datetime.datetime.now()) + ' START => ' + str(hyper_params) )\n",
    "\n",
    "        # Train the model\n",
    "        \n",
    "        model = fasttext.train_supervised(\"tweets.train\", **hyper_params)\n",
    "        print(\"Model trained with the hyperparameter \\n {}\".format(hyper_params))\n",
    "\n",
    "        # CHECK PERFORMANCE\n",
    "        \n",
    "        print(str(datetime.datetime.now()) + 'Training complete.' + str(hyper_params) )\n",
    "        \n",
    "        result = model.test('tweets.train')\n",
    "        validation = model.test('tweets.validate')\n",
    "        \n",
    "        # DISPLAY ACCURACY OF TRAINED MODEL\n",
    "        \n",
    "        text_line = str(hyper_params) + \"\\n\\nTraining Accuracy: \" + str(result[1] * 100)  + \" %\\nValidation Accuracy: \" + str(validation[1] * 100) + ' %\\n' \n",
    "        print(text_line)        \n",
    "    \n",
    "        #  TESTING PART\n",
    "\n",
    "        print('Testing\\n')\n",
    "        sample_text = 'this player does not play well'\n",
    "        print_test_prediction(model, sample_text)\n",
    "        sample_text = 'this player is soooo shit'\n",
    "        print_test_prediction(model, sample_text)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Exception during training: ' + str(e) )\n",
    "\n",
    "\n",
    "# Train your model\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
