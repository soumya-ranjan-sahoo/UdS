{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "lTDK2BUZHigw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NNIA 18/19 Project 2:  Gradient Descent & Backpropagation"
      ]
    },
    {
      "metadata": {
        "id": "7RU7DjFIHig9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Deadline: 4. January 2018, 23:59"
      ]
    },
    {
      "metadata": {
        "id": "ZfDGN9GFHihC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Multinomial Logistic Regression and Cross Validation $~$ (12 points)"
      ]
    },
    {
      "metadata": {
        "id": "up5SeKa7HihL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this exercise, you will implement a [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) model with tensorflow for Fashion-MNIST dataset. Cross Validation will be used to find the best **regularization parameter** $\\lambda$ for the L2-regularization term. Fashion-MNIST dataset is similar to the sklearn Digit dataset you used in the Project 1. It contains 60,000 training images and 10,000 testing images. Each example is a 28×28 grayscale image, associated with a label from 10 classes."
      ]
    },
    {
      "metadata": {
        "id": "EloT10V6HihO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Neural Network](https://s3-eu-central-1.amazonaws.com/zalando-wp-zalando-research-production/2017/08/fashion-mnist-sprite.png)"
      ]
    },
    {
      "metadata": {
        "id": "i-3vITRMHihR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Multinomial logistic regression is a probabilistic, linear classifier. It is parametrized by a weight matrix $W$ and a bias vector $b$. Classification is done by projecting an input vector onto a set of hyperplanes, each of which corresponds to a class. The distance from the input to a hyperplane reflects the probability that the input is a member of the corresponding class.\n",
        "\n",
        "Mathematically, the probability that an input vector $\\bf{x} \\in \\mathbb{R}^p$ is a member of a class $i$ can be written as:\n",
        "$$P(Y=i|\\textbf{x}, W, b) = softmax(W\\textbf{x} + b)_i = \\frac{e^{W_i\\textbf{x} + b_i}}{\\sum_j{e^{W_j\\textbf{x} + b_j}}}$$\n",
        "where $W \\in \\mathbb{R}^{c \\times p}$, $b \\in \\mathbb{R}^c$ and $W_i \\in \\mathbb{R}^p$.\n",
        "\n",
        "The model’s prediction $y_{pred}$ is the class whose probability is maximal, specifically:\n",
        "$$y_{pred} = argmax_iP(Y=i|\\textbf{x}, W, b)$$\n",
        "\n",
        "We use cross-entropy loss with L2 regularization."
      ]
    },
    {
      "metadata": {
        "id": "sPQsheQFHihW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1 Dataset and Normalization"
      ]
    },
    {
      "metadata": {
        "id": "K0-P8SZQHiha",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load **Fashion-MNIST** dataset and normalized it."
      ]
    },
    {
      "metadata": {
        "id": "Iiask7z4Hihf",
        "colab_type": "code",
        "outputId": "4a523cfa-6429-4521-ec63-38ca54b56301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import struct\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(X_trainval, Y_trainval), (X_test, Y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "tSnB8P7_Hihp",
        "colab_type": "code",
        "outputId": "feaaf2a7-a1cd-403d-a421-3a5d782a1fe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "X_trainval = np.reshape(X_trainval, (X_trainval.shape[0],  X_trainval.shape[1] *  X_trainval.shape[2]))\n",
        "print('The X_trainval has the following shape:')\n",
        "print('Rows: %d, columns: %d' % (X_trainval.shape[0], X_trainval.shape[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The X_trainval has the following shape:\n",
            "Rows: 60000, columns: 784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OMnWY35UHih2",
        "colab_type": "code",
        "outputId": "4bfa792e-378b-4a85-fbc6-a9af50b01daa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "X_test = np.reshape(X_test, (X_test.shape[0],  X_test.shape[1] *  X_test.shape[2]))\n",
        "print('The X_test has the following shape:')\n",
        "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The X_test has the following shape:\n",
            "Rows: 10000, columns: 784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mXgtYePbUD3M",
        "colab_type": "code",
        "outputId": "71af072e-02c4-4c79-d503-426e0bc3c34c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "Y_trainval=tf.keras.utils.to_categorical(Y_trainval)\n",
        "Y_test=tf.keras.utils.to_categorical(Y_test)\n",
        "print(Y_trainval.shape,Y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 10) (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yP-A--6DHiiB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Normalize the data. Subtract the mean and divide by the standard deviation."
      ]
    },
    {
      "metadata": {
        "id": "rWrCh77kHiiE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def data_normalization(X_trainval, X_test):\n",
        "    X_trainval_normalised= X_trainval-np.mean(X_trainval,axis=0)/ np.std(X_trainval,axis=0)\n",
        "    X_test_normalised= X_test-np.mean(X_test,axis=0)/ np.std(X_test,axis=0)\n",
        "    return X_trainval_normalised, X_test_normalised"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7p8yrT4WHiiU",
        "colab_type": "code",
        "outputId": "127c4988-62d3-48c2-b0fb-64d3e76fb21d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# The normalization should be done on X_train and X_test. \n",
        "# The normalized data should have the exactly same shape as the original data matrix.\n",
        "\n",
        "X_trainval, X_test = data_normalization(X_trainval, X_test)\n",
        "print(X_trainval.shape,X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784) (10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DEH1g70tHiii",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Points:** $0.0$ of $1.0$\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "zHwJfjD3Hiim",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.2 Define the Computation Graph"
      ]
    },
    {
      "metadata": {
        "id": "AffjVFjuHiio",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Here the global configuration of this program is \n",
        "# defined, which you shouldn't change.\n",
        "\n",
        "class global_config(object):\n",
        "    lr = 0.0001  # learning rate\n",
        "    img_h = 28  # image height\n",
        "    img_w = 28  # image width\n",
        "    num_class = 10  # number of classes\n",
        "    num_epoch = 20  # number of training epochs\n",
        "    batch_size = 16  # batch size\n",
        "    K = 3  # K-fold cross validation\n",
        "    num_train = None  # the number of training data\n",
        "    lambd = None  # the factor for the L2-regularization\n",
        "\n",
        "config = global_config()\n",
        "config.num_train = X_trainval.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nA0Tg5KLHii7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_val_split(X_trainval, Y_trainval, i, K):\n",
        "    \"\"\"\n",
        "    sklearn library is not allowed to use here.\n",
        "    \n",
        "    K is the total number of folds and i is the current fold.\n",
        "    \n",
        "    Think about how to deal with the case when the number of \n",
        "    training data can't be divided by K evenly.\n",
        "    \"\"\"\n",
        "    #TODO: Implement\n",
        "    #when N cant be divided by K, N%K splits can get an extra data point each?\n",
        "    #what is i? the most recent fold used for val? or the starting fold of training data?\n",
        "    N=X_trainval.shape[0]\n",
        "    n=int(N/K)\n",
        "    if N%K > 0:\n",
        "      s=N%K*(n+1)\n",
        "      split_indicies=np.concatenate((np.arange(0,s,(n+1)),np.arange(s,N+1,n)))\n",
        "    else:\n",
        "      split_indicies=np.arange(0,N+1,n)\n",
        "      \n",
        "    X_train=np.concatenate((X_trainval[split_indicies[0]:split_indicies[i]],X_trainval[split_indicies[i+1]:]))\n",
        "    Y_train=np.concatenate((Y_trainval[split_indicies[0]:split_indicies[i]],Y_trainval[split_indicies[i+1]:]))\n",
        "    \n",
        "    X_val=X_trainval[split_indicies[i]:split_indicies[i+1]]\n",
        "    Y_val=Y_trainval[split_indicies[i]:split_indicies[i+1]]\n",
        "    \n",
        "    return X_train, X_val, Y_train, Y_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3YtHHjPoHijH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Points:** $0.0$ of $2.0$\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "_1C9LorAHijL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def shuffle_train_data(X_train, Y_train):\n",
        "    \"\"\"called after each epoch\"\"\"\n",
        "    perm = np.random.permutation(len(Y_train))\n",
        "    Xtr_shuf = X_train[perm]\n",
        "    Ytr_shuf = Y_train[perm]\n",
        "    return Xtr_shuf, Ytr_shuf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_CyQX_nMHijY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "training\n",
        "\"\"\"\n",
        "class logistic_regression(object):\n",
        "    \n",
        "    def __init__(self, X, Y_gt, config, name):\n",
        "        \"\"\"\n",
        "        :param X: the training batch, which has the shape [batch_size, n_features].\n",
        "        :param Y_gt: the corresponding ground truth label vector.\n",
        "        :param config: the hyper-parameters you need for the implementation.\n",
        "        :param name: the name of this logistic regression model which is used to\n",
        "                     avoid the naming confict with the help of tf.variable_scope and reuse.\n",
        "       \n",
        "        Define the computation graph within the variable_scope here. \n",
        "        First define two variables W and b with tf.get_variable.\n",
        "        Then do the forward pass.\n",
        "        Then compute the cross entropy loss with tensorflow, don't forget the L2-regularization.\n",
        "        The Adam optimizer is already given. You shouldn't change it.\n",
        "        Finally compute the accuracy for one batch\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
        "            \n",
        "            #TODO: Define two variables and the forward pass.\n",
        "            n_features=X.shape[1]\n",
        "            W=tf.get_variable('weights',shape=[n_features,config.num_class],dtype=tf.float32,initializer=tf.zeros_initializer())\n",
        "            b=tf.get_variable('biases',shape=[config.num_class],initializer=tf.zeros_initializer())\n",
        "#             y = tf.nn.softmax(tf.matmul(X, W) + b)\n",
        "            y = tf.matmul(X, W) + b            \n",
        "             \n",
        "            #TODO: Compute the cross entropy loss with L2-regularization.\n",
        "            self._loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y,labels=Y_gt)+config.lambd*tf.nn.l2_loss(W))\n",
        "            \n",
        "            \n",
        "            # Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent \n",
        "            # to update network weights iteratively.\n",
        "            # It will be introduced in the lecture when talking about the optimization algorithms.\n",
        "            self._train_step = tf.train.AdamOptimizer(config.lr).minimize(self._loss)\n",
        "            \n",
        "            #TODO: Compute the accuracy\n",
        "            pred = tf.nn.softmax(y)\n",
        "            correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(Y_gt,1))\n",
        "            self._num_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "            \n",
        "    @property\n",
        "    def train_op(self):\n",
        "        return self._train_step\n",
        "    \n",
        "    @property\n",
        "    def loss(self):\n",
        "        return self._loss\n",
        "    \n",
        "    @property\n",
        "    def num_acc(self):\n",
        "        return self._num_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gp8azmAqHijj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Points:** $0.0$ of $2.0$\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "2vPinY_Z9jY6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def get_batch(x,y,config):\n",
        "#   dataset = tf.data.Dataset.from_tensor_slices((x,y)) \n",
        "#   batched_dataset = dataset.batch(config.batch_size)\n",
        "#   batch_iter = batched_dataset.make_one_shot_iterator()\n",
        "#   return batch_iter.get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v_P9L2X05oI5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # unit test for making minibatches\n",
        "# bx,by=get_batch(X_trainval,Y_trainval,config)\n",
        "# print(bx,by)\n",
        "\n",
        "# with tf.Session() as sess:\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "#   batch_X,batch_Y=sess.run([bx,by])\n",
        "# print(batch_X.shape,batch_Y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pOR2OSIqHijp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def testing(model, X_test, Y_test, config):\n",
        "    \"\"\" \n",
        "    Go through the X_test and use sess.run() to compute the loss and accuracy.\n",
        "    \n",
        "    Return the total loss and the accuracy for X_test.\n",
        "    \n",
        "    Note that this function will be used for the validation data\n",
        "    during training and the test data after training.\n",
        "    \"\"\"\n",
        "    num_test = X_test.shape[0]\n",
        "    total_cost = 0.\n",
        "    accs = 0.\n",
        "  \n",
        "    total_cost, accs = sess.run([model.loss, model.num_acc],feed_dict={X: X_test, Y_gt: Y_test})\n",
        "    \n",
        "#     return total_cost / len(Y_test), accs / len(Y_test)\n",
        "    return total_cost, accs\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dl-JGOibHij3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Points:** $0.0$ of $2.0$\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "yvyWLe5IHij6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, X_train, X_val, Y_train, Y_val, config):\n",
        "    \"\"\"\n",
        "    Train the model with sess.run().\n",
        "    \n",
        "    You should shuffle the data after each epoch and\n",
        "    evaluate training and validation loss after each epoch.\n",
        "    \n",
        "    Return the lists of the training/validation loss and accuracy.\n",
        "    \"\"\"\n",
        "    cost_trains = []\n",
        "    acc_trains = []\n",
        "    cost_vals = []\n",
        "    acc_vals = []\n",
        "       \n",
        "    \n",
        "    for i in range(config.num_epoch):\n",
        "       #TODO: Implement\n",
        "        cost_train=0.\n",
        "        acc_train=0.\n",
        "        num_batches=int(X_train.shape[0]/config.batch_size)\n",
        "        start=0\n",
        "#         print(num_batches)\n",
        "#         bx,by=get_batch(X_train,Y_train,config)\n",
        "      \n",
        "                \n",
        "        for b in range(num_batches):\n",
        "            batch_X = X_train[start:start+config.batch_size]\n",
        "            batch_Y = Y_train[start:start+config.batch_size]\n",
        "            start=start+config.batch_size\n",
        "#             batch_X,batch_Y=sess.run([bx,by])\n",
        "#             print(batch_Y.shape)            \n",
        "            _, batch_error = sess.run([model.train_op, model.loss],feed_dict={X: batch_X, Y_gt: batch_Y})\n",
        "           \n",
        "            batch_acc=sess.run(model.num_acc,feed_dict={X: batch_X, Y_gt: batch_Y})\n",
        "            cost_train += batch_error\n",
        "            acc_train += batch_acc\n",
        "        cost_train /= num_batches\n",
        "        acc_train /= num_batches\n",
        "                \n",
        "        cost_val,acc_val=testing(model, X_test, Y_test, config)\n",
        "        \n",
        "        X_train,Y_train=shuffle_train_data(X_train, Y_train)\n",
        "        X_val,Y_val=shuffle_train_data(X_val, Y_val)\n",
        "        \n",
        "        \n",
        "        cost_trains.append(cost_train)\n",
        "        acc_trains.append(acc_train)\n",
        "        print(\"Epoch: %d :\" % (i + 1))\n",
        "        print(\"Train Loss: %f\" %  cost_train)\n",
        "        print(\"Training acc: %f\" % acc_train)\n",
        "        cost_vals.append(cost_val)\n",
        "        acc_vals.append(acc_val)\n",
        "        print(\"Validation Loss: %f\" % cost_val)\n",
        "        print(\"Validation acc: %f\" % acc_val)\n",
        "    return cost_trains, acc_trains, cost_vals, acc_vals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8OINJpt1HikD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Points:** $0.0$ of $2.0$\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "7epjAmwJHikH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3 Cross Validation"
      ]
    },
    {
      "metadata": {
        "id": "VxqDEQ9oHikQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Implement cross validation to find an optimal value of $\\lambda$. The optimal hyper-parameters should be determined by the validation accuracy. The test set should only be used in the very end after all other processing, e.g. hyper-parameter choosing."
      ]
    },
    {
      "metadata": {
        "id": "TL9e0uQ_Hikb",
        "colab_type": "code",
        "outputId": "4b1341ce-e66e-43f8-e3fa-b00e7aaac6ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 15238
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Initialization\n",
        "\"\"\"\n",
        "# Use cross validation to choose the best lambda for the L2-regularization from the list below\n",
        "lambda_list = [100, 1, 0.1]\n",
        "\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, config.img_h * config.img_w])\n",
        "Y_gt = tf.placeholder(tf.float32, [None, config.num_class ])\n",
        "\n",
        "for lambd in lambda_list:\n",
        "    val_loss_list = []\n",
        "    config.lambd = lambd\n",
        "    print(\"lambda is %f\" % lambd)\n",
        "    \n",
        "    for i in range(config.K):\n",
        "        # Prepare the training and validation data\n",
        "        X_train, X_val, Y_train, Y_val = train_val_split(X_trainval, Y_trainval, i, config.K)\n",
        "        \n",
        "        # For each lambda and K, we build a new model and train it from scratch\n",
        "        model = logistic_regression(X, Y_gt, config, name=str(lambd)+'_'+str(config.K))\n",
        "        \n",
        "        with tf.Session() as sess:\n",
        "            \n",
        "            # Initialize the variables of the model\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            \n",
        "            # Train the model\n",
        "            cost_trains, acc_trains, cost_vals, acc_vals = train(model, X_train, X_val, Y_train, Y_val, config)\n",
        "            \n",
        "        val_loss_list.append(cost_vals[-1])\n",
        "        \n",
        "    print(\"The validation loss for lambda %f is %f\" % (lambd, np.mean(val_loss_list)))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lambda is 100.000000\n",
            "Epoch: 1 :\n",
            "Train Loss: 1.559821\n",
            "Training acc: 0.770900\n",
            "Validation Loss: 3.020351\n",
            "Validation acc: 0.644600\n",
            "Epoch: 2 :\n",
            "Train Loss: 1.514320\n",
            "Training acc: 0.781175\n",
            "Validation Loss: 1.130508\n",
            "Validation acc: 0.790700\n",
            "Epoch: 3 :\n",
            "Train Loss: 1.596525\n",
            "Training acc: 0.778700\n",
            "Validation Loss: 1.936035\n",
            "Validation acc: 0.698000\n",
            "Epoch: 4 :\n",
            "Train Loss: 1.597124\n",
            "Training acc: 0.776025\n",
            "Validation Loss: 1.704520\n",
            "Validation acc: 0.730100\n",
            "Epoch: 5 :\n",
            "Train Loss: 1.581060\n",
            "Training acc: 0.780025\n",
            "Validation Loss: 1.281757\n",
            "Validation acc: 0.772700\n",
            "Epoch: 6 :\n",
            "Train Loss: 1.540450\n",
            "Training acc: 0.781000\n",
            "Validation Loss: 3.136189\n",
            "Validation acc: 0.722300\n",
            "Epoch: 7 :\n",
            "Train Loss: 1.610114\n",
            "Training acc: 0.777925\n",
            "Validation Loss: 1.882398\n",
            "Validation acc: 0.704000\n",
            "Epoch: 8 :\n",
            "Train Loss: 1.553679\n",
            "Training acc: 0.780825\n",
            "Validation Loss: 2.577926\n",
            "Validation acc: 0.679200\n",
            "Epoch: 9 :\n",
            "Train Loss: 1.636597\n",
            "Training acc: 0.777975\n",
            "Validation Loss: 1.791513\n",
            "Validation acc: 0.734300\n",
            "Epoch: 10 :\n",
            "Train Loss: 1.640321\n",
            "Training acc: 0.776425\n",
            "Validation Loss: 3.087822\n",
            "Validation acc: 0.715300\n",
            "Epoch: 11 :\n",
            "Train Loss: 1.569419\n",
            "Training acc: 0.780100\n",
            "Validation Loss: 1.341873\n",
            "Validation acc: 0.751200\n",
            "Epoch: 12 :\n",
            "Train Loss: 1.581491\n",
            "Training acc: 0.782350\n",
            "Validation Loss: 1.532158\n",
            "Validation acc: 0.758500\n",
            "Epoch: 13 :\n",
            "Train Loss: 1.557629\n",
            "Training acc: 0.780875\n",
            "Validation Loss: 3.043087\n",
            "Validation acc: 0.649200\n",
            "Epoch: 14 :\n",
            "Train Loss: 1.585801\n",
            "Training acc: 0.776100\n",
            "Validation Loss: 1.023427\n",
            "Validation acc: 0.789000\n",
            "Epoch: 15 :\n",
            "Train Loss: 1.522251\n",
            "Training acc: 0.784125\n",
            "Validation Loss: 2.156433\n",
            "Validation acc: 0.653000\n",
            "Epoch: 16 :\n",
            "Train Loss: 1.534127\n",
            "Training acc: 0.780475\n",
            "Validation Loss: 1.164464\n",
            "Validation acc: 0.797700\n",
            "Epoch: 17 :\n",
            "Train Loss: 1.576865\n",
            "Training acc: 0.778300\n",
            "Validation Loss: 1.738735\n",
            "Validation acc: 0.725400\n",
            "Epoch: 18 :\n",
            "Train Loss: 1.608683\n",
            "Training acc: 0.779125\n",
            "Validation Loss: 1.319129\n",
            "Validation acc: 0.771300\n",
            "Epoch: 19 :\n",
            "Train Loss: 1.502359\n",
            "Training acc: 0.783325\n",
            "Validation Loss: 1.167353\n",
            "Validation acc: 0.781300\n",
            "Epoch: 20 :\n",
            "Train Loss: 1.556309\n",
            "Training acc: 0.779875\n",
            "Validation Loss: 1.778725\n",
            "Validation acc: 0.754200\n",
            "Epoch: 1 :\n",
            "Train Loss: 1.579577\n",
            "Training acc: 0.766600\n",
            "Validation Loss: 3.033214\n",
            "Validation acc: 0.643700\n",
            "Epoch: 2 :\n",
            "Train Loss: 1.570747\n",
            "Training acc: 0.775225\n",
            "Validation Loss: 1.584106\n",
            "Validation acc: 0.718700\n",
            "Epoch: 3 :\n",
            "Train Loss: 1.516876\n",
            "Training acc: 0.781600\n",
            "Validation Loss: 2.642662\n",
            "Validation acc: 0.652700\n",
            "Epoch: 4 :\n",
            "Train Loss: 1.579332\n",
            "Training acc: 0.779325\n",
            "Validation Loss: 1.579433\n",
            "Validation acc: 0.719300\n",
            "Epoch: 5 :\n",
            "Train Loss: 1.586637\n",
            "Training acc: 0.776675\n",
            "Validation Loss: 1.161636\n",
            "Validation acc: 0.784700\n",
            "Epoch: 6 :\n",
            "Train Loss: 1.564522\n",
            "Training acc: 0.776800\n",
            "Validation Loss: 1.277302\n",
            "Validation acc: 0.777600\n",
            "Epoch: 7 :\n",
            "Train Loss: 1.588244\n",
            "Training acc: 0.775675\n",
            "Validation Loss: 1.546607\n",
            "Validation acc: 0.733300\n",
            "Epoch: 8 :\n",
            "Train Loss: 1.585701\n",
            "Training acc: 0.776700\n",
            "Validation Loss: 1.981593\n",
            "Validation acc: 0.714100\n",
            "Epoch: 9 :\n",
            "Train Loss: 1.563586\n",
            "Training acc: 0.777775\n",
            "Validation Loss: 1.785620\n",
            "Validation acc: 0.687200\n",
            "Epoch: 10 :\n",
            "Train Loss: 1.589817\n",
            "Training acc: 0.777125\n",
            "Validation Loss: 1.604853\n",
            "Validation acc: 0.742800\n",
            "Epoch: 11 :\n",
            "Train Loss: 1.599003\n",
            "Training acc: 0.774725\n",
            "Validation Loss: 1.937347\n",
            "Validation acc: 0.722800\n",
            "Epoch: 12 :\n",
            "Train Loss: 1.588255\n",
            "Training acc: 0.777700\n",
            "Validation Loss: 1.800930\n",
            "Validation acc: 0.708700\n",
            "Epoch: 13 :\n",
            "Train Loss: 1.534694\n",
            "Training acc: 0.778950\n",
            "Validation Loss: 2.191007\n",
            "Validation acc: 0.680600\n",
            "Epoch: 14 :\n",
            "Train Loss: 1.549267\n",
            "Training acc: 0.778100\n",
            "Validation Loss: 1.713432\n",
            "Validation acc: 0.722400\n",
            "Epoch: 15 :\n",
            "Train Loss: 1.590598\n",
            "Training acc: 0.779900\n",
            "Validation Loss: 1.432050\n",
            "Validation acc: 0.755500\n",
            "Epoch: 16 :\n",
            "Train Loss: 1.532851\n",
            "Training acc: 0.779375\n",
            "Validation Loss: 1.328300\n",
            "Validation acc: 0.760500\n",
            "Epoch: 17 :\n",
            "Train Loss: 1.558221\n",
            "Training acc: 0.778375\n",
            "Validation Loss: 1.501373\n",
            "Validation acc: 0.740100\n",
            "Epoch: 18 :\n",
            "Train Loss: 1.598493\n",
            "Training acc: 0.776800\n",
            "Validation Loss: 1.356135\n",
            "Validation acc: 0.756300\n",
            "Epoch: 19 :\n",
            "Train Loss: 1.522961\n",
            "Training acc: 0.778925\n",
            "Validation Loss: 2.344179\n",
            "Validation acc: 0.677200\n",
            "Epoch: 20 :\n",
            "Train Loss: 1.561802\n",
            "Training acc: 0.781250\n",
            "Validation Loss: 1.432130\n",
            "Validation acc: 0.748300\n",
            "Epoch: 1 :\n",
            "Train Loss: 1.505348\n",
            "Training acc: 0.771300\n",
            "Validation Loss: 1.609822\n",
            "Validation acc: 0.721700\n",
            "Epoch: 2 :\n",
            "Train Loss: 1.581267\n",
            "Training acc: 0.778100\n",
            "Validation Loss: 1.445934\n",
            "Validation acc: 0.727700\n",
            "Epoch: 3 :\n",
            "Train Loss: 1.614843\n",
            "Training acc: 0.774575\n",
            "Validation Loss: 2.356636\n",
            "Validation acc: 0.685400\n",
            "Epoch: 4 :\n",
            "Train Loss: 1.589333\n",
            "Training acc: 0.776075\n",
            "Validation Loss: 1.686567\n",
            "Validation acc: 0.710200\n",
            "Epoch: 5 :\n",
            "Train Loss: 1.559189\n",
            "Training acc: 0.781475\n",
            "Validation Loss: 1.945626\n",
            "Validation acc: 0.705200\n",
            "Epoch: 6 :\n",
            "Train Loss: 1.566709\n",
            "Training acc: 0.778850\n",
            "Validation Loss: 1.482842\n",
            "Validation acc: 0.688600\n",
            "Epoch: 7 :\n",
            "Train Loss: 1.585613\n",
            "Training acc: 0.777600\n",
            "Validation Loss: 1.632406\n",
            "Validation acc: 0.741100\n",
            "Epoch: 8 :\n",
            "Train Loss: 1.547859\n",
            "Training acc: 0.780775\n",
            "Validation Loss: 1.644109\n",
            "Validation acc: 0.760300\n",
            "Epoch: 9 :\n",
            "Train Loss: 1.577301\n",
            "Training acc: 0.779225\n",
            "Validation Loss: 1.396610\n",
            "Validation acc: 0.756200\n",
            "Epoch: 10 :\n",
            "Train Loss: 1.614027\n",
            "Training acc: 0.774900\n",
            "Validation Loss: 1.905782\n",
            "Validation acc: 0.740900\n",
            "Epoch: 11 :\n",
            "Train Loss: 1.578374\n",
            "Training acc: 0.779125\n",
            "Validation Loss: 1.206814\n",
            "Validation acc: 0.774400\n",
            "Epoch: 12 :\n",
            "Train Loss: 1.539672\n",
            "Training acc: 0.778425\n",
            "Validation Loss: 1.909535\n",
            "Validation acc: 0.665100\n",
            "Epoch: 13 :\n",
            "Train Loss: 1.570717\n",
            "Training acc: 0.779475\n",
            "Validation Loss: 1.273218\n",
            "Validation acc: 0.750300\n",
            "Epoch: 14 :\n",
            "Train Loss: 1.543622\n",
            "Training acc: 0.778050\n",
            "Validation Loss: 1.433560\n",
            "Validation acc: 0.748300\n",
            "Epoch: 15 :\n",
            "Train Loss: 1.550526\n",
            "Training acc: 0.780050\n",
            "Validation Loss: 1.368539\n",
            "Validation acc: 0.752000\n",
            "Epoch: 16 :\n",
            "Train Loss: 1.537320\n",
            "Training acc: 0.778375\n",
            "Validation Loss: 1.559365\n",
            "Validation acc: 0.755300\n",
            "Epoch: 17 :\n",
            "Train Loss: 1.581980\n",
            "Training acc: 0.777775\n",
            "Validation Loss: 1.465649\n",
            "Validation acc: 0.755500\n",
            "Epoch: 18 :\n",
            "Train Loss: 1.547482\n",
            "Training acc: 0.780600\n",
            "Validation Loss: 2.000115\n",
            "Validation acc: 0.672200\n",
            "Epoch: 19 :\n",
            "Train Loss: 1.633584\n",
            "Training acc: 0.775775\n",
            "Validation Loss: 1.706143\n",
            "Validation acc: 0.727800\n",
            "Epoch: 20 :\n",
            "Train Loss: 1.599192\n",
            "Training acc: 0.777675\n",
            "Validation Loss: 1.389427\n",
            "Validation acc: 0.764200\n",
            "The validation loss for lambda 100.000000 is 1.533427\n",
            "lambda is 1.000000\n",
            "Epoch: 1 :\n",
            "Train Loss: 1.387639\n",
            "Training acc: 0.792800\n",
            "Validation Loss: 1.657150\n",
            "Validation acc: 0.787900\n",
            "Epoch: 2 :\n",
            "Train Loss: 1.384088\n",
            "Training acc: 0.819350\n",
            "Validation Loss: 1.863172\n",
            "Validation acc: 0.772000\n",
            "Epoch: 3 :\n",
            "Train Loss: 1.407128\n",
            "Training acc: 0.822075\n",
            "Validation Loss: 1.504061\n",
            "Validation acc: 0.790200\n",
            "Epoch: 4 :\n",
            "Train Loss: 1.442567\n",
            "Training acc: 0.823050\n",
            "Validation Loss: 2.267907\n",
            "Validation acc: 0.729900\n",
            "Epoch: 5 :\n",
            "Train Loss: 1.446913\n",
            "Training acc: 0.826575\n",
            "Validation Loss: 1.373554\n",
            "Validation acc: 0.811400\n",
            "Epoch: 6 :\n",
            "Train Loss: 1.442570\n",
            "Training acc: 0.826075\n",
            "Validation Loss: 1.484868\n",
            "Validation acc: 0.780900\n",
            "Epoch: 7 :\n",
            "Train Loss: 1.449347\n",
            "Training acc: 0.828850\n",
            "Validation Loss: 1.739815\n",
            "Validation acc: 0.770900\n",
            "Epoch: 8 :\n",
            "Train Loss: 1.457046\n",
            "Training acc: 0.826400\n",
            "Validation Loss: 1.566998\n",
            "Validation acc: 0.790000\n",
            "Epoch: 9 :\n",
            "Train Loss: 1.492385\n",
            "Training acc: 0.829125\n",
            "Validation Loss: 2.155623\n",
            "Validation acc: 0.771200\n",
            "Epoch: 10 :\n",
            "Train Loss: 1.442203\n",
            "Training acc: 0.830750\n",
            "Validation Loss: 1.644638\n",
            "Validation acc: 0.808900\n",
            "Epoch: 11 :\n",
            "Train Loss: 1.411124\n",
            "Training acc: 0.830925\n",
            "Validation Loss: 1.927401\n",
            "Validation acc: 0.755500\n",
            "Epoch: 12 :\n",
            "Train Loss: 1.457275\n",
            "Training acc: 0.829100\n",
            "Validation Loss: 1.702960\n",
            "Validation acc: 0.787400\n",
            "Epoch: 13 :\n",
            "Train Loss: 1.469419\n",
            "Training acc: 0.829500\n",
            "Validation Loss: 1.898878\n",
            "Validation acc: 0.774600\n",
            "Epoch: 14 :\n",
            "Train Loss: 1.458699\n",
            "Training acc: 0.830625\n",
            "Validation Loss: 1.700981\n",
            "Validation acc: 0.790800\n",
            "Epoch: 15 :\n",
            "Train Loss: 1.487759\n",
            "Training acc: 0.829650\n",
            "Validation Loss: 1.634532\n",
            "Validation acc: 0.785000\n",
            "Epoch: 16 :\n",
            "Train Loss: 1.426394\n",
            "Training acc: 0.833725\n",
            "Validation Loss: 1.453723\n",
            "Validation acc: 0.799200\n",
            "Epoch: 17 :\n",
            "Train Loss: 1.462325\n",
            "Training acc: 0.831650\n",
            "Validation Loss: 1.769807\n",
            "Validation acc: 0.794100\n",
            "Epoch: 18 :\n",
            "Train Loss: 1.442032\n",
            "Training acc: 0.831475\n",
            "Validation Loss: 1.837919\n",
            "Validation acc: 0.790700\n",
            "Epoch: 19 :\n",
            "Train Loss: 1.414863\n",
            "Training acc: 0.834400\n",
            "Validation Loss: 2.472377\n",
            "Validation acc: 0.737400\n",
            "Epoch: 20 :\n",
            "Train Loss: 1.455505\n",
            "Training acc: 0.832300\n",
            "Validation Loss: 1.820433\n",
            "Validation acc: 0.798300\n",
            "Epoch: 1 :\n",
            "Train Loss: 1.442164\n",
            "Training acc: 0.789200\n",
            "Validation Loss: 2.261633\n",
            "Validation acc: 0.776600\n",
            "Epoch: 2 :\n",
            "Train Loss: 1.436072\n",
            "Training acc: 0.811925\n",
            "Validation Loss: 1.659860\n",
            "Validation acc: 0.775400\n",
            "Epoch: 3 :\n",
            "Train Loss: 1.436692\n",
            "Training acc: 0.819450\n",
            "Validation Loss: 2.430187\n",
            "Validation acc: 0.703900\n",
            "Epoch: 4 :\n",
            "Train Loss: 1.443664\n",
            "Training acc: 0.821425\n",
            "Validation Loss: 1.732582\n",
            "Validation acc: 0.786900\n",
            "Epoch: 5 :\n",
            "Train Loss: 1.454737\n",
            "Training acc: 0.822000\n",
            "Validation Loss: 1.619064\n",
            "Validation acc: 0.806000\n",
            "Epoch: 6 :\n",
            "Train Loss: 1.437123\n",
            "Training acc: 0.823725\n",
            "Validation Loss: 2.227308\n",
            "Validation acc: 0.752400\n",
            "Epoch: 7 :\n",
            "Train Loss: 1.513261\n",
            "Training acc: 0.825525\n",
            "Validation Loss: 1.507002\n",
            "Validation acc: 0.789200\n",
            "Epoch: 8 :\n",
            "Train Loss: 1.466217\n",
            "Training acc: 0.827300\n",
            "Validation Loss: 1.926710\n",
            "Validation acc: 0.759000\n",
            "Epoch: 9 :\n",
            "Train Loss: 1.460665\n",
            "Training acc: 0.826800\n",
            "Validation Loss: 1.368556\n",
            "Validation acc: 0.814900\n",
            "Epoch: 10 :\n",
            "Train Loss: 1.430371\n",
            "Training acc: 0.827450\n",
            "Validation Loss: 1.500933\n",
            "Validation acc: 0.806400\n",
            "Epoch: 11 :\n",
            "Train Loss: 1.515667\n",
            "Training acc: 0.822900\n",
            "Validation Loss: 1.652339\n",
            "Validation acc: 0.789100\n",
            "Epoch: 12 :\n",
            "Train Loss: 1.466785\n",
            "Training acc: 0.827325\n",
            "Validation Loss: 1.707082\n",
            "Validation acc: 0.777900\n",
            "Epoch: 13 :\n",
            "Train Loss: 1.468966\n",
            "Training acc: 0.827850\n",
            "Validation Loss: 1.898476\n",
            "Validation acc: 0.745300\n",
            "Epoch: 14 :\n",
            "Train Loss: 1.507405\n",
            "Training acc: 0.826350\n",
            "Validation Loss: 2.025271\n",
            "Validation acc: 0.750900\n",
            "Epoch: 15 :\n",
            "Train Loss: 1.490198\n",
            "Training acc: 0.828375\n",
            "Validation Loss: 1.952352\n",
            "Validation acc: 0.791400\n",
            "Epoch: 16 :\n",
            "Train Loss: 1.476849\n",
            "Training acc: 0.830575\n",
            "Validation Loss: 3.684752\n",
            "Validation acc: 0.759700\n",
            "Epoch: 17 :\n",
            "Train Loss: 1.450081\n",
            "Training acc: 0.829100\n",
            "Validation Loss: 1.818700\n",
            "Validation acc: 0.786000\n",
            "Epoch: 18 :\n",
            "Train Loss: 1.576363\n",
            "Training acc: 0.825125\n",
            "Validation Loss: 1.765307\n",
            "Validation acc: 0.793000\n",
            "Epoch: 19 :\n",
            "Train Loss: 1.493526\n",
            "Training acc: 0.828500\n",
            "Validation Loss: 2.350685\n",
            "Validation acc: 0.746200\n",
            "Epoch: 20 :\n",
            "Train Loss: 1.543533\n",
            "Training acc: 0.825925\n",
            "Validation Loss: 1.434065\n",
            "Validation acc: 0.821600\n",
            "Epoch: 1 :\n",
            "Train Loss: 1.370860\n",
            "Training acc: 0.791050\n",
            "Validation Loss: 1.353256\n",
            "Validation acc: 0.789300\n",
            "Epoch: 2 :\n",
            "Train Loss: 1.408432\n",
            "Training acc: 0.815200\n",
            "Validation Loss: 1.897260\n",
            "Validation acc: 0.732100\n",
            "Epoch: 3 :\n",
            "Train Loss: 1.421632\n",
            "Training acc: 0.817600\n",
            "Validation Loss: 1.787370\n",
            "Validation acc: 0.763700\n",
            "Epoch: 4 :\n",
            "Train Loss: 1.486608\n",
            "Training acc: 0.818400\n",
            "Validation Loss: 1.682111\n",
            "Validation acc: 0.774900\n",
            "Epoch: 5 :\n",
            "Train Loss: 1.486423\n",
            "Training acc: 0.823075\n",
            "Validation Loss: 1.588008\n",
            "Validation acc: 0.772100\n",
            "Epoch: 6 :\n",
            "Train Loss: 1.466145\n",
            "Training acc: 0.822575\n",
            "Validation Loss: 1.828160\n",
            "Validation acc: 0.789800\n",
            "Epoch: 7 :\n",
            "Train Loss: 1.475287\n",
            "Training acc: 0.825100\n",
            "Validation Loss: 1.767720\n",
            "Validation acc: 0.777900\n",
            "Epoch: 8 :\n",
            "Train Loss: 1.419800\n",
            "Training acc: 0.826150\n",
            "Validation Loss: 1.348644\n",
            "Validation acc: 0.808400\n",
            "Epoch: 9 :\n",
            "Train Loss: 1.416267\n",
            "Training acc: 0.827200\n",
            "Validation Loss: 2.049914\n",
            "Validation acc: 0.735900\n",
            "Epoch: 10 :\n",
            "Train Loss: 1.474086\n",
            "Training acc: 0.826200\n",
            "Validation Loss: 1.454034\n",
            "Validation acc: 0.807800\n",
            "Epoch: 11 :\n",
            "Train Loss: 1.498072\n",
            "Training acc: 0.827825\n",
            "Validation Loss: 3.527048\n",
            "Validation acc: 0.747000\n",
            "Epoch: 12 :\n",
            "Train Loss: 1.442773\n",
            "Training acc: 0.828925\n",
            "Validation Loss: 1.595658\n",
            "Validation acc: 0.798200\n",
            "Epoch: 13 :\n",
            "Train Loss: 1.500817\n",
            "Training acc: 0.824900\n",
            "Validation Loss: 1.482657\n",
            "Validation acc: 0.802200\n",
            "Epoch: 14 :\n",
            "Train Loss: 1.447555\n",
            "Training acc: 0.829775\n",
            "Validation Loss: 1.714368\n",
            "Validation acc: 0.771100\n",
            "Epoch: 15 :\n",
            "Train Loss: 1.432731\n",
            "Training acc: 0.828975\n",
            "Validation Loss: 1.837228\n",
            "Validation acc: 0.778600\n",
            "Epoch: 16 :\n",
            "Train Loss: 1.498399\n",
            "Training acc: 0.828225\n",
            "Validation Loss: 1.535560\n",
            "Validation acc: 0.805500\n",
            "Epoch: 17 :\n",
            "Train Loss: 1.444827\n",
            "Training acc: 0.828075\n",
            "Validation Loss: 1.417811\n",
            "Validation acc: 0.812700\n",
            "Epoch: 18 :\n",
            "Train Loss: 1.467724\n",
            "Training acc: 0.830225\n",
            "Validation Loss: 1.913044\n",
            "Validation acc: 0.775400\n",
            "Epoch: 19 :\n",
            "Train Loss: 1.448972\n",
            "Training acc: 0.831300\n",
            "Validation Loss: 2.413866\n",
            "Validation acc: 0.736800\n",
            "Epoch: 20 :\n",
            "Train Loss: 1.493251\n",
            "Training acc: 0.827550\n",
            "Validation Loss: 1.799114\n",
            "Validation acc: 0.778400\n",
            "The validation loss for lambda 1.000000 is 1.684538\n",
            "lambda is 0.100000\n",
            "Epoch: 1 :\n",
            "Train Loss: 1.378931\n",
            "Training acc: 0.793725\n",
            "Validation Loss: 1.499143\n",
            "Validation acc: 0.790900\n",
            "Epoch: 2 :\n",
            "Train Loss: 1.418725\n",
            "Training acc: 0.819225\n",
            "Validation Loss: 2.213621\n",
            "Validation acc: 0.742000\n",
            "Epoch: 3 :\n",
            "Train Loss: 1.402239\n",
            "Training acc: 0.821850\n",
            "Validation Loss: 1.395463\n",
            "Validation acc: 0.793600\n",
            "Epoch: 4 :\n",
            "Train Loss: 1.395752\n",
            "Training acc: 0.824150\n",
            "Validation Loss: 1.801434\n",
            "Validation acc: 0.773700\n",
            "Epoch: 5 :\n",
            "Train Loss: 1.404268\n",
            "Training acc: 0.830275\n",
            "Validation Loss: 1.780710\n",
            "Validation acc: 0.794300\n",
            "Epoch: 6 :\n",
            "Train Loss: 1.406912\n",
            "Training acc: 0.829575\n",
            "Validation Loss: 1.582975\n",
            "Validation acc: 0.805300\n",
            "Epoch: 7 :\n",
            "Train Loss: 1.390883\n",
            "Training acc: 0.831150\n",
            "Validation Loss: 1.489305\n",
            "Validation acc: 0.805700\n",
            "Epoch: 8 :\n",
            "Train Loss: 1.455179\n",
            "Training acc: 0.834450\n",
            "Validation Loss: 1.918825\n",
            "Validation acc: 0.796700\n",
            "Epoch: 9 :\n",
            "Train Loss: 1.410660\n",
            "Training acc: 0.834625\n",
            "Validation Loss: 1.967334\n",
            "Validation acc: 0.786000\n",
            "Epoch: 10 :\n",
            "Train Loss: 1.436794\n",
            "Training acc: 0.835100\n",
            "Validation Loss: 1.952754\n",
            "Validation acc: 0.780800\n",
            "Epoch: 11 :\n",
            "Train Loss: 1.416769\n",
            "Training acc: 0.837800\n",
            "Validation Loss: 1.739859\n",
            "Validation acc: 0.804200\n",
            "Epoch: 12 :\n",
            "Train Loss: 1.413269\n",
            "Training acc: 0.837050\n",
            "Validation Loss: 3.221627\n",
            "Validation acc: 0.737700\n",
            "Epoch: 13 :\n",
            "Train Loss: 1.354149\n",
            "Training acc: 0.840050\n",
            "Validation Loss: 1.562187\n",
            "Validation acc: 0.811600\n",
            "Epoch: 14 :\n",
            "Train Loss: 1.428052\n",
            "Training acc: 0.835325\n",
            "Validation Loss: 1.875768\n",
            "Validation acc: 0.782300\n",
            "Epoch: 15 :\n",
            "Train Loss: 1.407361\n",
            "Training acc: 0.837075\n",
            "Validation Loss: 1.729475\n",
            "Validation acc: 0.792800\n",
            "Epoch: 16 :\n",
            "Train Loss: 1.387787\n",
            "Training acc: 0.838550\n",
            "Validation Loss: 1.514517\n",
            "Validation acc: 0.816600\n",
            "Epoch: 17 :\n",
            "Train Loss: 1.391764\n",
            "Training acc: 0.838125\n",
            "Validation Loss: 2.567746\n",
            "Validation acc: 0.729700\n",
            "Epoch: 18 :\n",
            "Train Loss: 1.381811\n",
            "Training acc: 0.840850\n",
            "Validation Loss: 1.599664\n",
            "Validation acc: 0.812200\n",
            "Epoch: 19 :\n",
            "Train Loss: 1.408887\n",
            "Training acc: 0.838550\n",
            "Validation Loss: 1.553586\n",
            "Validation acc: 0.815100\n",
            "Epoch: 20 :\n",
            "Train Loss: 1.383177\n",
            "Training acc: 0.841525\n",
            "Validation Loss: 1.965921\n",
            "Validation acc: 0.781200\n",
            "Epoch: 1 :\n",
            "Train Loss: 1.400773\n",
            "Training acc: 0.790600\n",
            "Validation Loss: 1.814758\n",
            "Validation acc: 0.759700\n",
            "Epoch: 2 :\n",
            "Train Loss: 1.406228\n",
            "Training acc: 0.812750\n",
            "Validation Loss: 1.482440\n",
            "Validation acc: 0.785900\n",
            "Epoch: 3 :\n",
            "Train Loss: 1.430951\n",
            "Training acc: 0.818650\n",
            "Validation Loss: 1.594478\n",
            "Validation acc: 0.785500\n",
            "Epoch: 4 :\n",
            "Train Loss: 1.447715\n",
            "Training acc: 0.823200\n",
            "Validation Loss: 2.176786\n",
            "Validation acc: 0.729900\n",
            "Epoch: 5 :\n",
            "Train Loss: 1.444955\n",
            "Training acc: 0.823675\n",
            "Validation Loss: 2.055121\n",
            "Validation acc: 0.774800\n",
            "Epoch: 6 :\n",
            "Train Loss: 1.375730\n",
            "Training acc: 0.831550\n",
            "Validation Loss: 1.239956\n",
            "Validation acc: 0.817600\n",
            "Epoch: 7 :\n",
            "Train Loss: 1.416134\n",
            "Training acc: 0.829575\n",
            "Validation Loss: 1.422636\n",
            "Validation acc: 0.805500\n",
            "Epoch: 8 :\n",
            "Train Loss: 1.444278\n",
            "Training acc: 0.828350\n",
            "Validation Loss: 2.263260\n",
            "Validation acc: 0.752700\n",
            "Epoch: 9 :\n",
            "Train Loss: 1.395637\n",
            "Training acc: 0.831750\n",
            "Validation Loss: 1.641814\n",
            "Validation acc: 0.783200\n",
            "Epoch: 10 :\n",
            "Train Loss: 1.438685\n",
            "Training acc: 0.829875\n",
            "Validation Loss: 1.919374\n",
            "Validation acc: 0.762600\n",
            "Epoch: 11 :\n",
            "Train Loss: 1.410539\n",
            "Training acc: 0.833775\n",
            "Validation Loss: 1.690477\n",
            "Validation acc: 0.793100\n",
            "Epoch: 12 :\n",
            "Train Loss: 1.441088\n",
            "Training acc: 0.831375\n",
            "Validation Loss: 2.132061\n",
            "Validation acc: 0.779900\n",
            "Epoch: 13 :\n",
            "Train Loss: 1.436743\n",
            "Training acc: 0.832625\n",
            "Validation Loss: 1.523141\n",
            "Validation acc: 0.799300\n",
            "Epoch: 14 :\n",
            "Train Loss: 1.485317\n",
            "Training acc: 0.833975\n",
            "Validation Loss: 1.978346\n",
            "Validation acc: 0.782200\n",
            "Epoch: 15 :\n",
            "Train Loss: 1.437187\n",
            "Training acc: 0.834175\n",
            "Validation Loss: 2.077691\n",
            "Validation acc: 0.783000\n",
            "Epoch: 16 :\n",
            "Train Loss: 1.374061\n",
            "Training acc: 0.837325\n",
            "Validation Loss: 1.984600\n",
            "Validation acc: 0.778800\n",
            "Epoch: 17 :\n",
            "Train Loss: 1.418888\n",
            "Training acc: 0.835800\n",
            "Validation Loss: 1.844987\n",
            "Validation acc: 0.782400\n",
            "Epoch: 18 :\n",
            "Train Loss: 1.421677\n",
            "Training acc: 0.837250\n",
            "Validation Loss: 1.722242\n",
            "Validation acc: 0.800200\n",
            "Epoch: 19 :\n",
            "Train Loss: 1.431505\n",
            "Training acc: 0.835550\n",
            "Validation Loss: 1.541300\n",
            "Validation acc: 0.817300\n",
            "Epoch: 20 :\n",
            "Train Loss: 1.403834\n",
            "Training acc: 0.839000\n",
            "Validation Loss: 1.829080\n",
            "Validation acc: 0.790900\n",
            "Epoch: 1 :\n",
            "Train Loss: 1.351206\n",
            "Training acc: 0.791975\n",
            "Validation Loss: 1.399118\n",
            "Validation acc: 0.786100\n",
            "Epoch: 2 :\n",
            "Train Loss: 1.428849\n",
            "Training acc: 0.814700\n",
            "Validation Loss: 1.392144\n",
            "Validation acc: 0.804000\n",
            "Epoch: 3 :\n",
            "Train Loss: 1.462564\n",
            "Training acc: 0.819000\n",
            "Validation Loss: 1.328557\n",
            "Validation acc: 0.811900\n",
            "Epoch: 4 :\n",
            "Train Loss: 1.400685\n",
            "Training acc: 0.823800\n",
            "Validation Loss: 2.234657\n",
            "Validation acc: 0.766800\n",
            "Epoch: 5 :\n",
            "Train Loss: 1.445279\n",
            "Training acc: 0.825200\n",
            "Validation Loss: 1.546042\n",
            "Validation acc: 0.800700\n",
            "Epoch: 6 :\n",
            "Train Loss: 1.424878\n",
            "Training acc: 0.827975\n",
            "Validation Loss: 1.668249\n",
            "Validation acc: 0.803200\n",
            "Epoch: 7 :\n",
            "Train Loss: 1.388748\n",
            "Training acc: 0.830950\n",
            "Validation Loss: 2.037254\n",
            "Validation acc: 0.790100\n",
            "Epoch: 8 :\n",
            "Train Loss: 1.428994\n",
            "Training acc: 0.830575\n",
            "Validation Loss: 1.393150\n",
            "Validation acc: 0.809800\n",
            "Epoch: 9 :\n",
            "Train Loss: 1.440621\n",
            "Training acc: 0.829350\n",
            "Validation Loss: 1.622789\n",
            "Validation acc: 0.779200\n",
            "Epoch: 10 :\n",
            "Train Loss: 1.405151\n",
            "Training acc: 0.833625\n",
            "Validation Loss: 1.791595\n",
            "Validation acc: 0.786200\n",
            "Epoch: 11 :\n",
            "Train Loss: 1.467553\n",
            "Training acc: 0.831800\n",
            "Validation Loss: 1.601767\n",
            "Validation acc: 0.810200\n",
            "Epoch: 12 :\n",
            "Train Loss: 1.429903\n",
            "Training acc: 0.835300\n",
            "Validation Loss: 1.752826\n",
            "Validation acc: 0.800500\n",
            "Epoch: 13 :\n",
            "Train Loss: 1.428007\n",
            "Training acc: 0.834875\n",
            "Validation Loss: 2.480350\n",
            "Validation acc: 0.733500\n",
            "Epoch: 14 :\n",
            "Train Loss: 1.421265\n",
            "Training acc: 0.833350\n",
            "Validation Loss: 1.738404\n",
            "Validation acc: 0.791400\n",
            "Epoch: 15 :\n",
            "Train Loss: 1.412249\n",
            "Training acc: 0.834925\n",
            "Validation Loss: 1.901509\n",
            "Validation acc: 0.786500\n",
            "Epoch: 16 :\n",
            "Train Loss: 1.450423\n",
            "Training acc: 0.833700\n",
            "Validation Loss: 1.494453\n",
            "Validation acc: 0.819000\n",
            "Epoch: 17 :\n",
            "Train Loss: 1.475944\n",
            "Training acc: 0.836250\n",
            "Validation Loss: 1.805602\n",
            "Validation acc: 0.792900\n",
            "Epoch: 18 :\n",
            "Train Loss: 1.433208\n",
            "Training acc: 0.836750\n",
            "Validation Loss: 1.544446\n",
            "Validation acc: 0.807900\n",
            "Epoch: 19 :\n",
            "Train Loss: 1.395294\n",
            "Training acc: 0.838650\n",
            "Validation Loss: 1.476913\n",
            "Validation acc: 0.805800\n",
            "Epoch: 20 :\n",
            "Train Loss: 1.443759\n",
            "Training acc: 0.834650\n",
            "Validation Loss: 2.053017\n",
            "Validation acc: 0.796700\n",
            "The validation loss for lambda 0.100000 is 1.949339\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yOZPSX6I6w5p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y93ii3ESHikp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.4 Combine Train and Validation data."
      ]
    },
    {
      "metadata": {
        "id": "vK6qfdjmHikr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Use the hyper-parameters you choose from the cross validation to re-train the model."
      ]
    },
    {
      "metadata": {
        "id": "DjKr2YSkHiku",
        "colab_type": "code",
        "outputId": "bfab7615-8a7e-4f28-e03f-88c3f28da892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1714
        }
      },
      "cell_type": "code",
      "source": [
        "config.lambd = 100  #TODO: Choose the best lambda\n",
        "model = logistic_regression(X, Y_gt, config, name='trainval')\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    cost_trains, acc_trains, cost_tests, acc_tests = train(model, X_trainval, X_test, Y_trainval, Y_test, config)\n",
        "\n",
        "print(\"The final test acc is %f\" % acc_tests[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 :\n",
            "Train Loss: 1.532274\n",
            "Training acc: 0.773750\n",
            "Validation Loss: 3.722109\n",
            "Validation acc: 0.650000\n",
            "Epoch: 2 :\n",
            "Train Loss: 1.580752\n",
            "Training acc: 0.778033\n",
            "Validation Loss: 1.688266\n",
            "Validation acc: 0.739900\n",
            "Epoch: 3 :\n",
            "Train Loss: 1.576943\n",
            "Training acc: 0.777783\n",
            "Validation Loss: 1.437472\n",
            "Validation acc: 0.756100\n",
            "Epoch: 4 :\n",
            "Train Loss: 1.578922\n",
            "Training acc: 0.778767\n",
            "Validation Loss: 1.484006\n",
            "Validation acc: 0.742100\n",
            "Epoch: 5 :\n",
            "Train Loss: 1.547667\n",
            "Training acc: 0.779417\n",
            "Validation Loss: 1.836202\n",
            "Validation acc: 0.701900\n",
            "Epoch: 6 :\n",
            "Train Loss: 1.611927\n",
            "Training acc: 0.775517\n",
            "Validation Loss: 1.457206\n",
            "Validation acc: 0.741500\n",
            "Epoch: 7 :\n",
            "Train Loss: 1.552892\n",
            "Training acc: 0.778367\n",
            "Validation Loss: 1.384164\n",
            "Validation acc: 0.748700\n",
            "Epoch: 8 :\n",
            "Train Loss: 1.606829\n",
            "Training acc: 0.777267\n",
            "Validation Loss: 1.255086\n",
            "Validation acc: 0.773700\n",
            "Epoch: 9 :\n",
            "Train Loss: 1.582176\n",
            "Training acc: 0.777767\n",
            "Validation Loss: 1.587537\n",
            "Validation acc: 0.744600\n",
            "Epoch: 10 :\n",
            "Train Loss: 1.573319\n",
            "Training acc: 0.778933\n",
            "Validation Loss: 1.967786\n",
            "Validation acc: 0.701700\n",
            "Epoch: 11 :\n",
            "Train Loss: 1.589790\n",
            "Training acc: 0.777367\n",
            "Validation Loss: 1.248101\n",
            "Validation acc: 0.773700\n",
            "Epoch: 12 :\n",
            "Train Loss: 1.555849\n",
            "Training acc: 0.779083\n",
            "Validation Loss: 1.480052\n",
            "Validation acc: 0.740700\n",
            "Epoch: 13 :\n",
            "Train Loss: 1.573543\n",
            "Training acc: 0.778567\n",
            "Validation Loss: 1.082013\n",
            "Validation acc: 0.789900\n",
            "Epoch: 14 :\n",
            "Train Loss: 1.556679\n",
            "Training acc: 0.779967\n",
            "Validation Loss: 1.411541\n",
            "Validation acc: 0.728900\n",
            "Epoch: 15 :\n",
            "Train Loss: 1.543366\n",
            "Training acc: 0.778600\n",
            "Validation Loss: 1.470779\n",
            "Validation acc: 0.753700\n",
            "Epoch: 16 :\n",
            "Train Loss: 1.638562\n",
            "Training acc: 0.777300\n",
            "Validation Loss: 1.607305\n",
            "Validation acc: 0.742800\n",
            "Epoch: 17 :\n",
            "Train Loss: 1.559742\n",
            "Training acc: 0.778817\n",
            "Validation Loss: 1.730645\n",
            "Validation acc: 0.759600\n",
            "Epoch: 18 :\n",
            "Train Loss: 1.527329\n",
            "Training acc: 0.782317\n",
            "Validation Loss: 1.608442\n",
            "Validation acc: 0.713700\n",
            "Epoch: 19 :\n",
            "Train Loss: 1.545927\n",
            "Training acc: 0.780367\n",
            "Validation Loss: 1.575715\n",
            "Validation acc: 0.723700\n",
            "Epoch: 20 :\n",
            "Train Loss: 1.535278\n",
            "Training acc: 0.780517\n",
            "Validation Loss: 1.523229\n",
            "Validation acc: 0.742600\n",
            "The final test acc is 0.742600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-cHDcm3HHik5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Points:** $0.0$ of $0.5$\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "544QPGqyHik7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.5 Questions"
      ]
    },
    {
      "metadata": {
        "id": "2go2DUC8HilQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. What is the impact of k in k-fold cross validation?\n",
        "\n",
        "2. What will happen to the training if you change the $\\lambda$ for L2-regularization?\n",
        "\n",
        "3. Why do we perform the gradient descent on a batch of the data rather than all of the data?\n",
        "\n",
        "4. Why does the loss increase, when the learning rate is too large?\n",
        "\n",
        "5. Do we apply L2-regularization for the bias $b$?"
      ]
    },
    {
      "metadata": {
        "id": "Gyt6DNxFHilY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Answer:* \n",
        "1. In K-fold validation, resampling strategy, K, is the number of groups we split the training samples into. The value of K should be wisely chosen such that it fits the bias-variance tradeoff. (With lower and Higher K respectively).\n",
        "\n",
        "2. As $\\lambda$ is inversely proportional to he norm of weights (w), increasing $\\lambda$ decreases w and hence penalizes large parameters. By adjusting $\\lambda$ we can have a better control on data fitting and hence avoid over-fittimg and under-fitting suitably. \n",
        "\n",
        "3.Gradient descent on batch of data allows memory efficient training schedules with less training samples in memory. Also, the model update frequency allows for a more robust convergence, avoiding local minima.\n",
        "\n",
        "4. If learning rate is too large, gradient descent might overshoot the minimum and will not converge, also, might diverge and hence increasing the loss.\n",
        "\n",
        "5. No, we usually skip the bias b during L2-regularization. Penalization of the bias(intercept) would make the procedure depend on the origin chosen for Y_Pred; that is, adding a constant c to each of the targets would not simply result in a shift of the predictions by the same amount c. "
      ]
    },
    {
      "metadata": {
        "id": "1XObU9PqHilb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Points:** $0.0$ of $2.5$\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "eyFcRl0fHile",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Getting to know Back-Propagation in details $~$ (18 points)"
      ]
    },
    {
      "metadata": {
        "id": "FuEF16J_Hilh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the following exercise you would build a **feed-forward network** from scratch using **only** Numpy. For this, you also have to implement **Back-propagation** in python. Additionally, this network should have the option of **L2 regularization** enabled within it."
      ]
    },
    {
      "metadata": {
        "id": "A0JF2CzfHilu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Before you start**: In this exercise you will implement a single hidden layer feedforward neural network. In case you are unfamiliar with the terminology and notation used here, please consult chapter 6 of the Deep Learning Book before you proceed."
      ]
    },
    {
      "metadata": {
        "id": "OY7RJv2hHil1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Generally speaking, a feedword neural network with a single hidden layer can be represented by the following function $$ f(x;\\theta) = f^{(2)}(f^{(1)}(f^{(0)}(x)))$$ where $f^{(0)}(x)$ is the input layer, $f^{(1)}(.)$ is the so called hidden layer, and $f^{(2)}(.)$ is the ouput layer of the network. $\\theta$ represents the parameters of the network whose values will be learned during the training phase."
      ]
    },
    {
      "metadata": {
        "id": "XEMcClmSHil9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The network that you will implement in this exercise has the following layers:\n",
        "* $f^{(0)}(x) = \\mathbf{X}$, with $\\mathbf{X} \\in \\mathbb{R}^{b,p}$ where $b$ is the batch size and $p$ is the number of features.\n",
        "* $f^{(1)}(.) = \\sigma(\\mathbf{X} \\mathbf{W_1}+b_1)$, with $\\mathbf{X} \\in \\mathbb{R}^{b, p}$, $\\mathbf{W_1} \\in \\mathbb{R}^{p,u_1}$, $\\textbf{b}_1 \\in \\mathbb{R}^{u_1}$ where $u_1$ is the number of **hidden units**. Additonally, $\\sigma(x) = \\frac{1}{1 + \\exp{(-x})}$ is the **sigmoid** function.\n",
        "* $f^{(2)}(.) = softmax(\\mathbf{X} \\mathbf{W_2}+b_2)$, with $\\mathbf{X} \\in \\mathbb{R}^{b, u_1}$, $\\mathbf{W_2} \\in \\mathbb{R}^{u_1,u_2}$, $\\textbf{b}_2 \\in \\mathbb{R}^{u_2}$ where $u_2$ is the number of **output classes** in this particular layer.\n",
        "\n",
        "Note that both, $\\sigma(x)$ are applied **elementwise**. Further, the addition with the bias vector is also applied **elementwise** to each row of the matrix $\\mathbf{X} \\mathbf{W}$."
      ]
    },
    {
      "metadata": {
        "id": "be5z54pMHimB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Fully_connected_Neural_Network(object):\n",
        "    \"\"\" Fully-connected neural network with one hidden layer.\n",
        "\n",
        "    Parameters\n",
        "    ------------\n",
        "    n_output : int\n",
        "        Number of class labels.\n",
        "        \n",
        "    n_features : int\n",
        "        Number of input features.\n",
        "        \n",
        "    n_hidden : int\n",
        "        Number of hidden units.\n",
        "        \n",
        "    l2 : float\n",
        "        regularization parameter\n",
        "        0 means no regularization\n",
        "        \n",
        "    epochs : int\n",
        "        One Epoch is when the entire dataset is passed forward and backward through the neural network only once.\n",
        "        \n",
        "    lr : float\n",
        "        Learning rate.\n",
        "        \n",
        "    batchsize : int\n",
        "        Total number of training examples present in a single batch.\n",
        "        \n",
        "\n",
        "    Attributes\n",
        "    -----------\n",
        "    w1 : array, shape = [n_features, n_hidden_units]\n",
        "        Weight matrix for input layer -> hidden layer.\n",
        "    w2 : array, shape = [n_hidden_units, n_output_units]\n",
        "        Weight matrix for hidden layer -> output layer.\n",
        "    b1 : array, shape = [n_hidden_units, ]\n",
        "        Bias for input layer-> hidden layer.\n",
        "    b2 : array, shape = [n_output_units, ]\n",
        "        Bias for hidden layer -> output layer.\n",
        "\n",
        "    \"\"\"\n",
        "    # Points: 2.0\n",
        "    def __init__(self, n_output, n_features, n_hidden=30,\n",
        "                 l2=0.0, epochs=50, lr=0.001, batchsize=1):\n",
        "        self.n_output = n_output\n",
        "        self.n_features = n_features\n",
        "        self.n_hidden = n_hidden\n",
        "        self.l2 = l2\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "        self.batchsize = batchsize\n",
        "        #TODO Initialize weights and biases with np.random.uniform or np.random.normal and specify the shape\n",
        "        self.w1 = np.random.uniform(-1, 1, (self.n_features,self.n_hidden))\n",
        "        self.w2 = np.random.uniform(-1, 1, (self.n_hidden,self.n_output))\n",
        "        \n",
        "        #self.w1 = np.random.uniform(low=0.0, high=1.0, size=self.n_features*self.n_hidden)\n",
        "        #self.w1 = self.w1.reshape(self.n_features,self.n_hidden)\n",
        "        #self.w2 = np.random.uniform(low=0.0, high=1.0, size=self.n_hidden*self.n_output)\n",
        "        #self.w2 = self.w2.reshape(self.n_hidden,self.n_output)\n",
        "        \n",
        "        self.b1 = np.random.uniform(low=0.0, high=1.0, size=self.n_hidden)\n",
        "        self.b2 = np.random.uniform(low=0.0, high=1.0, size=self.n_output,)\n",
        "        #print(self.w1)\n",
        "        \n",
        "        #return w1,w2,b1,b2\n",
        "\n",
        "        \n",
        "    # Points: 0.5\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Compute sigmoid function\"\"\"\n",
        "        #TODO Implement\n",
        "        sigm = 1./(1.+np.exp(-z))\n",
        "        return sigm\n",
        "        \n",
        "\n",
        "\n",
        "    # Points: 0.5\n",
        "    def sigmoid_gradient(self, z):\n",
        "        \"\"\"Compute gradient of the sigmoid function\"\"\"\n",
        "        #TODO Implement\n",
        "        \n",
        "        sigm_grad = self.sigmoid(z)*(1. - self.sigmoid(z))\n",
        "        return sigm_grad\n",
        "\n",
        "    \n",
        "    # Points: 1.0\n",
        "    def softmax(self, z):\n",
        "        \"\"\"Compute softmax function.\n",
        "        Implement a stable version which \n",
        "        takes care of overflow and underflow.\n",
        "        \"\"\"        \n",
        "        #TODO Implement\n",
        "        x = np.max(z,axis=1)\n",
        "        x = x[:, np.newaxis]\n",
        "        num = np.exp(z-x)\n",
        "        denom = np.sum(num, axis=1)\n",
        "        denom = denom[:, np.newaxis]\n",
        "        stable_softmax = num/denom\n",
        "#         print('computed softmax')\n",
        "        return stable_softmax\n",
        "\n",
        "        \n",
        "    # Points: 2.0\n",
        "    def forward(self, X):\n",
        "        \"\"\"Compute feedforward step\n",
        "\n",
        "        Parameters\n",
        "        -----------\n",
        "        X : array, shape = [n_samples, n_features]\n",
        "            \n",
        "        Returns\n",
        "        ----------\n",
        "        z2 : array,\n",
        "            Input of the hidden layer.\n",
        "        a2 : array,\n",
        "            Output of the hidden layer.\n",
        "        z3 : array,\n",
        "            Input of the output layer.\n",
        "        a3 : array,\n",
        "            Output of the output layer.\n",
        "\n",
        "        \"\"\"\n",
        "        # TODO Implement\n",
        "        \n",
        "        \n",
        "        \n",
        "       #z2 = np.sum(np.matmul(X,self.w1),self.b1)\n",
        "        z2 = np.matmul(X,self.w1) + self.b1\n",
        "        a2 = self.sigmoid(z2)\n",
        "       #z3 = np.sum(np.matmul(a2,self.w2)+self.b2)\n",
        "        z3 = np.matmul(a2,self.w2) + self.b2\n",
        "        a3 = self.softmax(z3)\n",
        "#         print('completed fprop')\n",
        "        \n",
        "        \n",
        "        return z2, a2, z3, a3\n",
        "        \n",
        "    # Points: 0.5\n",
        "    def L2_regularization(self, lambd):\n",
        "        \"\"\"Implement L2-regularization loss\"\"\"\n",
        "        #TODO Implement\n",
        "        l2_reg = (np.sum(np.square(self.w1)) + np.sum(np.square(self.w2)))*(lambd/(2*self.batchsize))\n",
        "        return l2_reg\n",
        "        \n",
        "       \n",
        "        \n",
        "        \n",
        "    # Points: 2.0\n",
        "    def loss(self, y_enc, output, epsilon=1e-12):\n",
        "        \"\"\"Implement total loss.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        y_enc : array, one-hot encoded labels.\n",
        "        \n",
        "        output : array, output of the output layer\n",
        "\n",
        "        Returns\n",
        "        ---------\n",
        "        cost : float, total loss.\n",
        "\n",
        "        \"\"\"\n",
        "        #TODO Implement\n",
        "        np.place(output,output == 0, epsilon)\n",
        "       #cost = - np.mean(np.sum(y_enc * np.log(output),axis = 1))\n",
        "        cost = - np.mean(np.sum(y_enc * np.log(output),axis = 1))+self.L2_regularization(epsilon)\n",
        "        #logprobs = np.multiply(y_enc, np.log(output)) + np.multiply((1 - y_enc), np.log(1 - output))\n",
        "        #cross_entropy_cost = (-1.0/self.n_output) * np.sum(logprobs) \n",
        "        #cost = cross_entropy_cost+self.L2_regularization(self.l2)\n",
        "        \n",
        "        return cost\n",
        "        \n",
        "        \n",
        "    # Points: 4.0\n",
        "    def compute_gradient(self, X, a2, a3, z2, y_enc):\n",
        "        \"\"\" Compute gradient using backpropagation.\n",
        "\n",
        "        Parameters\n",
        "        ------------\n",
        "        X : array, Input.\n",
        "        a2 : array, output of the hidden layer.\n",
        "        a3 : array, output of the output layer.\n",
        "        z2 : array, input of the hidden layer.\n",
        "        y_enc : array, one-hot encoded labels.\n",
        "\n",
        "        Returns\n",
        "        ---------\n",
        "        grad1 : array, Gradient of the weight matrix w1.\n",
        "        grad2 : array, Gradient of the weight matrix w2.\n",
        "        grad3 : array, Gradient of the bias vector b1.\n",
        "        grad4 : array, Gradient of the bias vector b2.\n",
        "        \"\"\"\n",
        "        #TODO Implement\n",
        "        m = self.batchsize\n",
        "       # print(m)\n",
        "        \n",
        "        #dz3 = a3 - y_enc\n",
        "       #grad1 = np.dot(np.transpose(X),(np.dot((a3-y_enc),np.transpose(self.w2)) * self.sigmoid_gradient(z2)))\n",
        "        #grad3 = np.dot((a3-y_enc),np.transpose(self.w2)) * self.sigmoid_gradient(z2) \n",
        "        \n",
        "        grad1 = np.dot(np.transpose(X),(np.dot((a3-y_enc),np.transpose(self.w2)) * self.sigmoid_gradient(z2))) + (self.l2/m)*self.w1 #dw1\n",
        "        grad3 = np.dot((a3-y_enc),np.transpose(self.w2)) * self.sigmoid_gradient(z2)  #db1\n",
        "        grad2 = np.dot(np.transpose(a2),(a3 - y_enc)) + (self.l2/m)*self.w2  #dw2\n",
        "        grad4 = a3 - y_enc #db2\n",
        "        \n",
        "        #dw2 = (1.0/m) * np.matmul(np.transpose(a2), dz3) ## add the regularization term ? #changed to a3\n",
        "        #db2 = (1.0/m) * np.sum(dz3, axis=1, keepdims=True)\n",
        "    \n",
        "        #dz2 = np.matmul(dz3, np.transpose(self.w2)) * (1 - np.power(a2, 2))\n",
        "        #dw1 = (1.0/m) * np.matmul(np.transpose(X),dz2) ## add the regularization ? \n",
        "        #db1 = (1.0/m) * np.sum(dz2, axis=1, keepdims=True)\n",
        "        \n",
        "        #grad1 = dw1\n",
        "        #grad2 = dw2\n",
        "        #grad3 = db1\n",
        "        #grad4 = db2 \n",
        "#         print('computed grads')\n",
        "\n",
        "        return grad1, grad2, grad3, grad4\n",
        "        \n",
        "    # Points: 1.0\n",
        "    def inference(self, X):\n",
        "        \"\"\"Predict class labels\n",
        "\n",
        "        Parameters\n",
        "        -----------\n",
        "        X : array, Input.\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        y_pred : array, Predicted labels.\n",
        "\n",
        "        \"\"\"\n",
        "        # TODO Implement\n",
        "        z2, a2, z3, a3 = self.forward(X)\n",
        "       # y_pred = a3 > 0.5\n",
        "        if a3 > 0.5 :\n",
        "          return 1\n",
        "        else:\n",
        "          return 0\n",
        "        \n",
        "       \n",
        "       # return y_pred\n",
        "    \n",
        "    \n",
        "    def shuffle_train_data(self, X, Y):\n",
        "        \"\"\"called after each epoch\"\"\"\n",
        "        perm = np.random.permutation(Y.shape[0])\n",
        "        X_shuf = X[perm]\n",
        "        Y_shuf = Y[perm]\n",
        "        return X_shuf, Y_shuf\n",
        "      \n",
        "    \n",
        "    \n",
        "    # Points: 2.0\n",
        "    def train(self, X_train, Y_train, verbose=False):\n",
        "        \"\"\" Fit the model.\n",
        "\n",
        "        Parameters\n",
        "        -----------\n",
        "        X : array, Input.\n",
        "        y : array, Ground truth class labels.\n",
        "        verbose : bool, Print the training progress\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        self\n",
        "\n",
        "        \"\"\"\n",
        "        #TODO Initialization\n",
        "        self.cost_ = []\n",
        "        self.batch_cost_list = []\n",
        "        \n",
        "\n",
        "        for i in range(self.epochs):\n",
        "        \n",
        "            if verbose:\n",
        "                print('\\nEpoch: %d/%d' % (i+1, self.epochs))\n",
        "\n",
        "\n",
        "                # feedforward and loss computation\n",
        "                num_batches=int(X_train.shape[0]/self.batchsize)\n",
        "                #print('fprop')\n",
        "                start=0\n",
        "                cost_train=0.\n",
        "                \n",
        "                \n",
        "                for b in range(num_batches):\n",
        "                  #print(\"batch_num\",b)\n",
        "                  batch_X = X_train[start:start+self.batchsize]\n",
        "                  batch_Y = Y_train[start:start+self.batchsize]\n",
        "                  start=start+self.batchsize\n",
        "                  z2, a2, z3, a3 = self.forward(batch_X)\n",
        "                  batch_cost =  self.loss(batch_Y, a3, self.l2)\n",
        "                  self.batch_cost_list.append(batch_cost)\n",
        "                  cost_train =  cost_train + batch_cost\n",
        "                  \n",
        "                  #print('grads')\n",
        "                  grad1, grad2, grad3, grad4 = self.compute_gradient(batch_X, a2, a3, z2, batch_Y)\n",
        "                  #Updating weights\n",
        "                  \n",
        "                  self.w1 = self.w1 - (self.lr*grad1)\n",
        "                  self.b1 = self.b1 - (self.lr*grad3).sum(axis=0)\n",
        "                  self.w2 = self.w2 - (self.lr*grad2)\n",
        "                  self.b2 = self.b2 - (self.lr*grad4).sum(axis=0)\n",
        "                  \n",
        "                  \n",
        "                cost = cost_train/num_batches\n",
        "                print(cost)\n",
        "                self.cost_.append(cost)\n",
        "\n",
        "                #compute gradient via backpropagation and update the weights\n",
        "                #print('grads')\n",
        "                #grad1, grad2, grad3, grad4 = self.compute_gradient(X_train, a2, a3, z2, Y_train)\n",
        "\n",
        "        return self\n",
        "      \n",
        "      \n",
        "      ##evaluation function for calculating the train and test accuracy\n",
        "    def evaluation (self, X, Y):\n",
        "      \n",
        "        \n",
        "      num_batches=int(X.shape[0]/self.batchsize)\n",
        "      start=0\n",
        "     # y_pred_list = []\n",
        "      acc_list = []\n",
        "      for b in range(num_batches):\n",
        "        counter = 0\n",
        "        batch_Y_list = []\n",
        "        a3_list = []\n",
        "        batch_X = X[start:start+self.batchsize]\n",
        "        batch_Y = Y[start:start+self.batchsize]\n",
        "        start=start+self.batchsize\n",
        "        z2, a2, z3, a3 = self.forward(batch_X)\n",
        "        a3_list.append(a3.argmax(1))\n",
        "        batch_Y_list.append(batch_Y.argmax(1))\n",
        "        #count = sum(np.equal(a3_list,batch_Y_list))\n",
        "       # print(a3_list.shape,batch_Y_list.shape)\n",
        "        #counter = np.sum(a3_list == batch_Y) ##element wise comparison\n",
        "        #print(count)\n",
        "        #print(len(a3))\n",
        "       # y_pred_list.append(a3-batch_Y)\n",
        "       # counter = np.count_nonzero(y_pred_list)\n",
        "        #print\n",
        "       # print(counter)\n",
        "        acc_list.append(np.mean(np.equal(a3_list,batch_Y_list)))\n",
        "      #acc_list = y_pred_list - Y\n",
        "      return np.mean(acc_list)\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Sb9esnyHimO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Points:** $0.0$ of $15.5$\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "pEQ4DVULHimQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nn = Fully_connected_Neural_Network(n_output=10, \n",
        "                                    n_features=X_trainval.shape[1], \n",
        "                                    n_hidden=50, \n",
        "                                    l2=0.1, \n",
        "                                    epochs=1000, \n",
        "                                    lr=0.001,\n",
        "                                    batchsize=1200) #batchsize being changed from 50 to 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xzVyeu0YHimq",
        "colab_type": "code",
        "outputId": "f2ed676a-2136-4654-d6cd-a6aa9bf28c02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50434
        }
      },
      "cell_type": "code",
      "source": [
        "nn_params = nn.train(X_trainval, Y_trainval, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1/1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:74: RuntimeWarning: overflow encountered in exp\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.2134505677501664\n",
            "\n",
            "Epoch: 2/1000\n",
            "1.6974156923825856\n",
            "\n",
            "Epoch: 3/1000\n",
            "1.6127755566179176\n",
            "\n",
            "Epoch: 4/1000\n",
            "1.5655986334963163\n",
            "\n",
            "Epoch: 5/1000\n",
            "1.580213384632455\n",
            "\n",
            "Epoch: 6/1000\n",
            "1.541096085856363\n",
            "\n",
            "Epoch: 7/1000\n",
            "1.5809679391230502\n",
            "\n",
            "Epoch: 8/1000\n",
            "1.513561253769968\n",
            "\n",
            "Epoch: 9/1000\n",
            "1.585140597094572\n",
            "\n",
            "Epoch: 10/1000\n",
            "1.5986663785078732\n",
            "\n",
            "Epoch: 11/1000\n",
            "1.5923143295146107\n",
            "\n",
            "Epoch: 12/1000\n",
            "1.5668860366965063\n",
            "\n",
            "Epoch: 13/1000\n",
            "1.5216383292743765\n",
            "\n",
            "Epoch: 14/1000\n",
            "1.5155143027150906\n",
            "\n",
            "Epoch: 15/1000\n",
            "1.4922422327499085\n",
            "\n",
            "Epoch: 16/1000\n",
            "1.5820368401374896\n",
            "\n",
            "Epoch: 17/1000\n",
            "1.6701397872790704\n",
            "\n",
            "Epoch: 18/1000\n",
            "1.606686390753071\n",
            "\n",
            "Epoch: 19/1000\n",
            "1.5920213191362016\n",
            "\n",
            "Epoch: 20/1000\n",
            "1.6163507265775885\n",
            "\n",
            "Epoch: 21/1000\n",
            "1.6621432832070124\n",
            "\n",
            "Epoch: 22/1000\n",
            "1.7767749107502109\n",
            "\n",
            "Epoch: 23/1000\n",
            "1.6772577301959677\n",
            "\n",
            "Epoch: 24/1000\n",
            "1.7172775946601269\n",
            "\n",
            "Epoch: 25/1000\n",
            "1.622584576950138\n",
            "\n",
            "Epoch: 26/1000\n",
            "1.6357057565985673\n",
            "\n",
            "Epoch: 27/1000\n",
            "1.5588101626872537\n",
            "\n",
            "Epoch: 28/1000\n",
            "1.5680603361544518\n",
            "\n",
            "Epoch: 29/1000\n",
            "1.5953135640744514\n",
            "\n",
            "Epoch: 30/1000\n",
            "1.5517441610135978\n",
            "\n",
            "Epoch: 31/1000\n",
            "1.6000125466618436\n",
            "\n",
            "Epoch: 32/1000\n",
            "1.6890111344344256\n",
            "\n",
            "Epoch: 33/1000\n",
            "1.656753222752409\n",
            "\n",
            "Epoch: 34/1000\n",
            "1.560383132316468\n",
            "\n",
            "Epoch: 35/1000\n",
            "1.5263172642311797\n",
            "\n",
            "Epoch: 36/1000\n",
            "1.5882593760468808\n",
            "\n",
            "Epoch: 37/1000\n",
            "1.5886051115065383\n",
            "\n",
            "Epoch: 38/1000\n",
            "1.5842317565472166\n",
            "\n",
            "Epoch: 39/1000\n",
            "1.5354343739308731\n",
            "\n",
            "Epoch: 40/1000\n",
            "1.5431584720299012\n",
            "\n",
            "Epoch: 41/1000\n",
            "1.539477152138868\n",
            "\n",
            "Epoch: 42/1000\n",
            "1.4336999256139507\n",
            "\n",
            "Epoch: 43/1000\n",
            "1.4781468323061622\n",
            "\n",
            "Epoch: 44/1000\n",
            "1.4910922939640765\n",
            "\n",
            "Epoch: 45/1000\n",
            "1.531167611335443\n",
            "\n",
            "Epoch: 46/1000\n",
            "1.5078484900597096\n",
            "\n",
            "Epoch: 47/1000\n",
            "1.4622968207884586\n",
            "\n",
            "Epoch: 48/1000\n",
            "1.453220043494847\n",
            "\n",
            "Epoch: 49/1000\n",
            "1.4815112506949146\n",
            "\n",
            "Epoch: 50/1000\n",
            "1.4989577283423876\n",
            "\n",
            "Epoch: 51/1000\n",
            "1.5028876278052592\n",
            "\n",
            "Epoch: 52/1000\n",
            "1.5427722972007405\n",
            "\n",
            "Epoch: 53/1000\n",
            "1.5099774991934496\n",
            "\n",
            "Epoch: 54/1000\n",
            "1.585618958428411\n",
            "\n",
            "Epoch: 55/1000\n",
            "1.54234983537403\n",
            "\n",
            "Epoch: 56/1000\n",
            "1.4908758991732807\n",
            "\n",
            "Epoch: 57/1000\n",
            "1.4992867397286969\n",
            "\n",
            "Epoch: 58/1000\n",
            "1.5561002992994335\n",
            "\n",
            "Epoch: 59/1000\n",
            "1.5281431197832933\n",
            "\n",
            "Epoch: 60/1000\n",
            "1.618925890814744\n",
            "\n",
            "Epoch: 61/1000\n",
            "1.610278898843123\n",
            "\n",
            "Epoch: 62/1000\n",
            "1.561014084983989\n",
            "\n",
            "Epoch: 63/1000\n",
            "1.5441983743962109\n",
            "\n",
            "Epoch: 64/1000\n",
            "1.6283102269307352\n",
            "\n",
            "Epoch: 65/1000\n",
            "1.593149161922497\n",
            "\n",
            "Epoch: 66/1000\n",
            "1.563621122025918\n",
            "\n",
            "Epoch: 67/1000\n",
            "1.532447694451729\n",
            "\n",
            "Epoch: 68/1000\n",
            "1.4606367703536736\n",
            "\n",
            "Epoch: 69/1000\n",
            "1.4526583447506014\n",
            "\n",
            "Epoch: 70/1000\n",
            "1.4864877552493343\n",
            "\n",
            "Epoch: 71/1000\n",
            "1.474641496806302\n",
            "\n",
            "Epoch: 72/1000\n",
            "1.493022780314662\n",
            "\n",
            "Epoch: 73/1000\n",
            "1.53481967054063\n",
            "\n",
            "Epoch: 74/1000\n",
            "1.5292286247210052\n",
            "\n",
            "Epoch: 75/1000\n",
            "1.5292811167604816\n",
            "\n",
            "Epoch: 76/1000\n",
            "1.5171723649503484\n",
            "\n",
            "Epoch: 77/1000\n",
            "1.5488572009798107\n",
            "\n",
            "Epoch: 78/1000\n",
            "1.5035411415370232\n",
            "\n",
            "Epoch: 79/1000\n",
            "1.4948214026050834\n",
            "\n",
            "Epoch: 80/1000\n",
            "1.4860340972309805\n",
            "\n",
            "Epoch: 81/1000\n",
            "1.516770355846732\n",
            "\n",
            "Epoch: 82/1000\n",
            "1.5225764863607227\n",
            "\n",
            "Epoch: 83/1000\n",
            "1.507573684522539\n",
            "\n",
            "Epoch: 84/1000\n",
            "1.512721079019223\n",
            "\n",
            "Epoch: 85/1000\n",
            "1.5313861881068005\n",
            "\n",
            "Epoch: 86/1000\n",
            "1.5183481123753766\n",
            "\n",
            "Epoch: 87/1000\n",
            "1.4914362386677222\n",
            "\n",
            "Epoch: 88/1000\n",
            "1.4586747968497604\n",
            "\n",
            "Epoch: 89/1000\n",
            "1.5056277263331503\n",
            "\n",
            "Epoch: 90/1000\n",
            "1.610634280264871\n",
            "\n",
            "Epoch: 91/1000\n",
            "1.587895902327927\n",
            "\n",
            "Epoch: 92/1000\n",
            "1.54705141230511\n",
            "\n",
            "Epoch: 93/1000\n",
            "1.559749581934669\n",
            "\n",
            "Epoch: 94/1000\n",
            "1.546391313129292\n",
            "\n",
            "Epoch: 95/1000\n",
            "1.623208230542076\n",
            "\n",
            "Epoch: 96/1000\n",
            "1.5751700261528976\n",
            "\n",
            "Epoch: 97/1000\n",
            "1.4868021338909916\n",
            "\n",
            "Epoch: 98/1000\n",
            "1.4903467764352734\n",
            "\n",
            "Epoch: 99/1000\n",
            "1.4626077855316506\n",
            "\n",
            "Epoch: 100/1000\n",
            "1.4486182939948247\n",
            "\n",
            "Epoch: 101/1000\n",
            "1.4580180210584173\n",
            "\n",
            "Epoch: 102/1000\n",
            "1.4469404742225904\n",
            "\n",
            "Epoch: 103/1000\n",
            "1.49901506510919\n",
            "\n",
            "Epoch: 104/1000\n",
            "1.4870028134655908\n",
            "\n",
            "Epoch: 105/1000\n",
            "1.4744657205469927\n",
            "\n",
            "Epoch: 106/1000\n",
            "1.4751924745266467\n",
            "\n",
            "Epoch: 107/1000\n",
            "1.5139749617135494\n",
            "\n",
            "Epoch: 108/1000\n",
            "1.5083442320467273\n",
            "\n",
            "Epoch: 109/1000\n",
            "1.5127566700663595\n",
            "\n",
            "Epoch: 110/1000\n",
            "1.4718140468071998\n",
            "\n",
            "Epoch: 111/1000\n",
            "1.5403054199150388\n",
            "\n",
            "Epoch: 112/1000\n",
            "1.598631736530895\n",
            "\n",
            "Epoch: 113/1000\n",
            "1.5515507850587518\n",
            "\n",
            "Epoch: 114/1000\n",
            "1.5025118292410466\n",
            "\n",
            "Epoch: 115/1000\n",
            "1.5415290399968287\n",
            "\n",
            "Epoch: 116/1000\n",
            "1.517737253200168\n",
            "\n",
            "Epoch: 117/1000\n",
            "1.5237975545800928\n",
            "\n",
            "Epoch: 118/1000\n",
            "1.5783275167860051\n",
            "\n",
            "Epoch: 119/1000\n",
            "1.5324583514993708\n",
            "\n",
            "Epoch: 120/1000\n",
            "1.5733792166734226\n",
            "\n",
            "Epoch: 121/1000\n",
            "1.5292078564906522\n",
            "\n",
            "Epoch: 122/1000\n",
            "1.5133168582666527\n",
            "\n",
            "Epoch: 123/1000\n",
            "1.5345565500099243\n",
            "\n",
            "Epoch: 124/1000\n",
            "1.5319148957312125\n",
            "\n",
            "Epoch: 125/1000\n",
            "1.5008056009563047\n",
            "\n",
            "Epoch: 126/1000\n",
            "1.518209023515434\n",
            "\n",
            "Epoch: 127/1000\n",
            "1.4981280409499758\n",
            "\n",
            "Epoch: 128/1000\n",
            "1.5120378891576798\n",
            "\n",
            "Epoch: 129/1000\n",
            "1.5127345582665461\n",
            "\n",
            "Epoch: 130/1000\n",
            "1.5332778346993385\n",
            "\n",
            "Epoch: 131/1000\n",
            "1.5028539621173307\n",
            "\n",
            "Epoch: 132/1000\n",
            "1.4842484035034311\n",
            "\n",
            "Epoch: 133/1000\n",
            "1.4835097771015862\n",
            "\n",
            "Epoch: 134/1000\n",
            "1.48480984697062\n",
            "\n",
            "Epoch: 135/1000\n",
            "1.4430161496729954\n",
            "\n",
            "Epoch: 136/1000\n",
            "1.4665182577530802\n",
            "\n",
            "Epoch: 137/1000\n",
            "1.4890110610335945\n",
            "\n",
            "Epoch: 138/1000\n",
            "1.4856566913649527\n",
            "\n",
            "Epoch: 139/1000\n",
            "1.4889398970312369\n",
            "\n",
            "Epoch: 140/1000\n",
            "1.5567255727471139\n",
            "\n",
            "Epoch: 141/1000\n",
            "1.502288364032396\n",
            "\n",
            "Epoch: 142/1000\n",
            "1.522452886535912\n",
            "\n",
            "Epoch: 143/1000\n",
            "1.5000694097263156\n",
            "\n",
            "Epoch: 144/1000\n",
            "1.5113365323557388\n",
            "\n",
            "Epoch: 145/1000\n",
            "1.524904516136629\n",
            "\n",
            "Epoch: 146/1000\n",
            "1.4905776304812128\n",
            "\n",
            "Epoch: 147/1000\n",
            "1.4808856939770387\n",
            "\n",
            "Epoch: 148/1000\n",
            "1.4824490631148441\n",
            "\n",
            "Epoch: 149/1000\n",
            "1.5771991896234792\n",
            "\n",
            "Epoch: 150/1000\n",
            "1.535803831620562\n",
            "\n",
            "Epoch: 151/1000\n",
            "1.50717770361736\n",
            "\n",
            "Epoch: 152/1000\n",
            "1.4835822324842411\n",
            "\n",
            "Epoch: 153/1000\n",
            "1.5052936463274482\n",
            "\n",
            "Epoch: 154/1000\n",
            "1.479799079321653\n",
            "\n",
            "Epoch: 155/1000\n",
            "1.4946247544990434\n",
            "\n",
            "Epoch: 156/1000\n",
            "1.4822644597160977\n",
            "\n",
            "Epoch: 157/1000\n",
            "1.5301908406458293\n",
            "\n",
            "Epoch: 158/1000\n",
            "1.4929607737076065\n",
            "\n",
            "Epoch: 159/1000\n",
            "1.4907347132312403\n",
            "\n",
            "Epoch: 160/1000\n",
            "1.5030311656602706\n",
            "\n",
            "Epoch: 161/1000\n",
            "1.4918792222383397\n",
            "\n",
            "Epoch: 162/1000\n",
            "1.484076121971895\n",
            "\n",
            "Epoch: 163/1000\n",
            "1.4846990840607859\n",
            "\n",
            "Epoch: 164/1000\n",
            "1.5005421393794125\n",
            "\n",
            "Epoch: 165/1000\n",
            "1.5251766604441834\n",
            "\n",
            "Epoch: 166/1000\n",
            "1.5038099060642045\n",
            "\n",
            "Epoch: 167/1000\n",
            "1.4945759100246736\n",
            "\n",
            "Epoch: 168/1000\n",
            "1.4982265595326718\n",
            "\n",
            "Epoch: 169/1000\n",
            "1.4785246178569884\n",
            "\n",
            "Epoch: 170/1000\n",
            "1.4598450934050795\n",
            "\n",
            "Epoch: 171/1000\n",
            "1.4515089743190668\n",
            "\n",
            "Epoch: 172/1000\n",
            "1.4896170057268145\n",
            "\n",
            "Epoch: 173/1000\n",
            "1.4995241634181855\n",
            "\n",
            "Epoch: 174/1000\n",
            "1.4705060366938603\n",
            "\n",
            "Epoch: 175/1000\n",
            "1.4752394075553943\n",
            "\n",
            "Epoch: 176/1000\n",
            "1.5326657058383089\n",
            "\n",
            "Epoch: 177/1000\n",
            "1.5340354898008923\n",
            "\n",
            "Epoch: 178/1000\n",
            "1.515889787140567\n",
            "\n",
            "Epoch: 179/1000\n",
            "1.5061101882202388\n",
            "\n",
            "Epoch: 180/1000\n",
            "1.5231856680195426\n",
            "\n",
            "Epoch: 181/1000\n",
            "1.4888871669145407\n",
            "\n",
            "Epoch: 182/1000\n",
            "1.5073625705743612\n",
            "\n",
            "Epoch: 183/1000\n",
            "1.5052049963980345\n",
            "\n",
            "Epoch: 184/1000\n",
            "1.51855539457224\n",
            "\n",
            "Epoch: 185/1000\n",
            "1.494495706351449\n",
            "\n",
            "Epoch: 186/1000\n",
            "1.522599076088164\n",
            "\n",
            "Epoch: 187/1000\n",
            "1.5211934737624895\n",
            "\n",
            "Epoch: 188/1000\n",
            "1.545214775979674\n",
            "\n",
            "Epoch: 189/1000\n",
            "1.5785246158224344\n",
            "\n",
            "Epoch: 190/1000\n",
            "1.5189132811250285\n",
            "\n",
            "Epoch: 191/1000\n",
            "1.5186568227104738\n",
            "\n",
            "Epoch: 192/1000\n",
            "1.5115314273553677\n",
            "\n",
            "Epoch: 193/1000\n",
            "1.4940520254734733\n",
            "\n",
            "Epoch: 194/1000\n",
            "1.5006748996602042\n",
            "\n",
            "Epoch: 195/1000\n",
            "1.4935567651105077\n",
            "\n",
            "Epoch: 196/1000\n",
            "1.5083495577322201\n",
            "\n",
            "Epoch: 197/1000\n",
            "1.4869000366391978\n",
            "\n",
            "Epoch: 198/1000\n",
            "1.531241684241659\n",
            "\n",
            "Epoch: 199/1000\n",
            "1.5211827886977594\n",
            "\n",
            "Epoch: 200/1000\n",
            "1.5361470625924516\n",
            "\n",
            "Epoch: 201/1000\n",
            "1.5265385664042057\n",
            "\n",
            "Epoch: 202/1000\n",
            "1.5366060986904104\n",
            "\n",
            "Epoch: 203/1000\n",
            "1.5636646154547822\n",
            "\n",
            "Epoch: 204/1000\n",
            "1.5931971681766666\n",
            "\n",
            "Epoch: 205/1000\n",
            "1.5460679371066128\n",
            "\n",
            "Epoch: 206/1000\n",
            "1.5393516884000917\n",
            "\n",
            "Epoch: 207/1000\n",
            "1.518784889691882\n",
            "\n",
            "Epoch: 208/1000\n",
            "1.5378470746867223\n",
            "\n",
            "Epoch: 209/1000\n",
            "1.5064722348584818\n",
            "\n",
            "Epoch: 210/1000\n",
            "1.5153913775280117\n",
            "\n",
            "Epoch: 211/1000\n",
            "1.5121425818849352\n",
            "\n",
            "Epoch: 212/1000\n",
            "1.5165236531842528\n",
            "\n",
            "Epoch: 213/1000\n",
            "1.5394932633879372\n",
            "\n",
            "Epoch: 214/1000\n",
            "1.5113913276420226\n",
            "\n",
            "Epoch: 215/1000\n",
            "1.5120041459351163\n",
            "\n",
            "Epoch: 216/1000\n",
            "1.5229183694545796\n",
            "\n",
            "Epoch: 217/1000\n",
            "1.5893027188866937\n",
            "\n",
            "Epoch: 218/1000\n",
            "1.6030501348153283\n",
            "\n",
            "Epoch: 219/1000\n",
            "1.590150048189586\n",
            "\n",
            "Epoch: 220/1000\n",
            "1.5478945662595895\n",
            "\n",
            "Epoch: 221/1000\n",
            "1.523820661377758\n",
            "\n",
            "Epoch: 222/1000\n",
            "1.534422588200666\n",
            "\n",
            "Epoch: 223/1000\n",
            "1.5537112990831048\n",
            "\n",
            "Epoch: 224/1000\n",
            "1.5514718753932955\n",
            "\n",
            "Epoch: 225/1000\n",
            "1.5251754177350134\n",
            "\n",
            "Epoch: 226/1000\n",
            "1.5315864720947394\n",
            "\n",
            "Epoch: 227/1000\n",
            "1.5662101763032805\n",
            "\n",
            "Epoch: 228/1000\n",
            "1.5336849463236606\n",
            "\n",
            "Epoch: 229/1000\n",
            "1.5429153771988426\n",
            "\n",
            "Epoch: 230/1000\n",
            "1.5298798526165103\n",
            "\n",
            "Epoch: 231/1000\n",
            "1.5747949872808769\n",
            "\n",
            "Epoch: 232/1000\n",
            "1.5765997453024365\n",
            "\n",
            "Epoch: 233/1000\n",
            "1.5720655985291256\n",
            "\n",
            "Epoch: 234/1000\n",
            "1.5326363897003261\n",
            "\n",
            "Epoch: 235/1000\n",
            "1.5378754774819976\n",
            "\n",
            "Epoch: 236/1000\n",
            "1.5342331126623836\n",
            "\n",
            "Epoch: 237/1000\n",
            "1.533817914239261\n",
            "\n",
            "Epoch: 238/1000\n",
            "1.5730460139862523\n",
            "\n",
            "Epoch: 239/1000\n",
            "1.5637717891260214\n",
            "\n",
            "Epoch: 240/1000\n",
            "1.5312041793896944\n",
            "\n",
            "Epoch: 241/1000\n",
            "1.5250277441138862\n",
            "\n",
            "Epoch: 242/1000\n",
            "1.5400606512976285\n",
            "\n",
            "Epoch: 243/1000\n",
            "1.5410422066263236\n",
            "\n",
            "Epoch: 244/1000\n",
            "1.5803869272198205\n",
            "\n",
            "Epoch: 245/1000\n",
            "1.6082392334346018\n",
            "\n",
            "Epoch: 246/1000\n",
            "1.5743235262462454\n",
            "\n",
            "Epoch: 247/1000\n",
            "1.5963811322615546\n",
            "\n",
            "Epoch: 248/1000\n",
            "1.5926522299210524\n",
            "\n",
            "Epoch: 249/1000\n",
            "1.5680062924729803\n",
            "\n",
            "Epoch: 250/1000\n",
            "1.5579440853389566\n",
            "\n",
            "Epoch: 251/1000\n",
            "1.5661467545309717\n",
            "\n",
            "Epoch: 252/1000\n",
            "1.5481763342570247\n",
            "\n",
            "Epoch: 253/1000\n",
            "1.5867834621016257\n",
            "\n",
            "Epoch: 254/1000\n",
            "1.6295757452023638\n",
            "\n",
            "Epoch: 255/1000\n",
            "1.6763470377400147\n",
            "\n",
            "Epoch: 256/1000\n",
            "1.6019154914445701\n",
            "\n",
            "Epoch: 257/1000\n",
            "1.6044482099271042\n",
            "\n",
            "Epoch: 258/1000\n",
            "1.6058137933344425\n",
            "\n",
            "Epoch: 259/1000\n",
            "1.5940043430422741\n",
            "\n",
            "Epoch: 260/1000\n",
            "1.6258852572385956\n",
            "\n",
            "Epoch: 261/1000\n",
            "1.5976099256670722\n",
            "\n",
            "Epoch: 262/1000\n",
            "1.5628284909609877\n",
            "\n",
            "Epoch: 263/1000\n",
            "1.5406125544581009\n",
            "\n",
            "Epoch: 264/1000\n",
            "1.5340031041667632\n",
            "\n",
            "Epoch: 265/1000\n",
            "1.5317378000767445\n",
            "\n",
            "Epoch: 266/1000\n",
            "1.5740596742256858\n",
            "\n",
            "Epoch: 267/1000\n",
            "1.5327651489103176\n",
            "\n",
            "Epoch: 268/1000\n",
            "1.5303133656745505\n",
            "\n",
            "Epoch: 269/1000\n",
            "1.5518430170326285\n",
            "\n",
            "Epoch: 270/1000\n",
            "1.5773286736946246\n",
            "\n",
            "Epoch: 271/1000\n",
            "1.6277162851048737\n",
            "\n",
            "Epoch: 272/1000\n",
            "1.648881545943826\n",
            "\n",
            "Epoch: 273/1000\n",
            "1.6624411433537798\n",
            "\n",
            "Epoch: 274/1000\n",
            "1.6113262161129547\n",
            "\n",
            "Epoch: 275/1000\n",
            "1.590599486684209\n",
            "\n",
            "Epoch: 276/1000\n",
            "1.5805360177172545\n",
            "\n",
            "Epoch: 277/1000\n",
            "1.6034762172104693\n",
            "\n",
            "Epoch: 278/1000\n",
            "1.5808569287171013\n",
            "\n",
            "Epoch: 279/1000\n",
            "1.5847770597530857\n",
            "\n",
            "Epoch: 280/1000\n",
            "1.608304892083328\n",
            "\n",
            "Epoch: 281/1000\n",
            "1.6298888208172828\n",
            "\n",
            "Epoch: 282/1000\n",
            "1.6436247916219844\n",
            "\n",
            "Epoch: 283/1000\n",
            "1.7187953673621268\n",
            "\n",
            "Epoch: 284/1000\n",
            "1.694568237974781\n",
            "\n",
            "Epoch: 285/1000\n",
            "1.6746130801760515\n",
            "\n",
            "Epoch: 286/1000\n",
            "1.6532182782962446\n",
            "\n",
            "Epoch: 287/1000\n",
            "1.655447253544662\n",
            "\n",
            "Epoch: 288/1000\n",
            "1.6635752265211756\n",
            "\n",
            "Epoch: 289/1000\n",
            "1.682744854688063\n",
            "\n",
            "Epoch: 290/1000\n",
            "1.6587970620181525\n",
            "\n",
            "Epoch: 291/1000\n",
            "1.6478057292932013\n",
            "\n",
            "Epoch: 292/1000\n",
            "1.6477705118833215\n",
            "\n",
            "Epoch: 293/1000\n",
            "1.644805640984497\n",
            "\n",
            "Epoch: 294/1000\n",
            "1.7083583651345797\n",
            "\n",
            "Epoch: 295/1000\n",
            "1.7164899022023923\n",
            "\n",
            "Epoch: 296/1000\n",
            "1.6892487167307573\n",
            "\n",
            "Epoch: 297/1000\n",
            "1.737344100256282\n",
            "\n",
            "Epoch: 298/1000\n",
            "1.7627042998474654\n",
            "\n",
            "Epoch: 299/1000\n",
            "1.7345512221865982\n",
            "\n",
            "Epoch: 300/1000\n",
            "1.7090314093337151\n",
            "\n",
            "Epoch: 301/1000\n",
            "1.710835737424473\n",
            "\n",
            "Epoch: 302/1000\n",
            "1.6798026262860544\n",
            "\n",
            "Epoch: 303/1000\n",
            "1.6644845880459487\n",
            "\n",
            "Epoch: 304/1000\n",
            "1.6515607633617124\n",
            "\n",
            "Epoch: 305/1000\n",
            "1.6445264924861516\n",
            "\n",
            "Epoch: 306/1000\n",
            "1.6293426296722888\n",
            "\n",
            "Epoch: 307/1000\n",
            "1.6434355789895354\n",
            "\n",
            "Epoch: 308/1000\n",
            "1.6613270764008192\n",
            "\n",
            "Epoch: 309/1000\n",
            "1.6602781346861422\n",
            "\n",
            "Epoch: 310/1000\n",
            "1.6649083176942794\n",
            "\n",
            "Epoch: 311/1000\n",
            "1.653962969201676\n",
            "\n",
            "Epoch: 312/1000\n",
            "1.6954299737372387\n",
            "\n",
            "Epoch: 313/1000\n",
            "1.6772690111944757\n",
            "\n",
            "Epoch: 314/1000\n",
            "1.662376258439153\n",
            "\n",
            "Epoch: 315/1000\n",
            "1.6732991554052128\n",
            "\n",
            "Epoch: 316/1000\n",
            "1.6487395794212267\n",
            "\n",
            "Epoch: 317/1000\n",
            "1.6583341352587195\n",
            "\n",
            "Epoch: 318/1000\n",
            "1.676086375892173\n",
            "\n",
            "Epoch: 319/1000\n",
            "1.6636090478971033\n",
            "\n",
            "Epoch: 320/1000\n",
            "1.6475777046773428\n",
            "\n",
            "Epoch: 321/1000\n",
            "1.6508658841425872\n",
            "\n",
            "Epoch: 322/1000\n",
            "1.6663645582171904\n",
            "\n",
            "Epoch: 323/1000\n",
            "1.6288786701080853\n",
            "\n",
            "Epoch: 324/1000\n",
            "1.6132844708299354\n",
            "\n",
            "Epoch: 325/1000\n",
            "1.643744290950708\n",
            "\n",
            "Epoch: 326/1000\n",
            "1.6061819162057318\n",
            "\n",
            "Epoch: 327/1000\n",
            "1.6069537795809412\n",
            "\n",
            "Epoch: 328/1000\n",
            "1.5960450794000514\n",
            "\n",
            "Epoch: 329/1000\n",
            "1.5963273779212659\n",
            "\n",
            "Epoch: 330/1000\n",
            "1.6340667484007432\n",
            "\n",
            "Epoch: 331/1000\n",
            "1.6514686854029788\n",
            "\n",
            "Epoch: 332/1000\n",
            "1.637516321387091\n",
            "\n",
            "Epoch: 333/1000\n",
            "1.674376234814348\n",
            "\n",
            "Epoch: 334/1000\n",
            "1.6373113249249842\n",
            "\n",
            "Epoch: 335/1000\n",
            "1.6411029263737071\n",
            "\n",
            "Epoch: 336/1000\n",
            "1.6550028920200166\n",
            "\n",
            "Epoch: 337/1000\n",
            "1.6517699666591443\n",
            "\n",
            "Epoch: 338/1000\n",
            "1.663126552143048\n",
            "\n",
            "Epoch: 339/1000\n",
            "1.644154173199286\n",
            "\n",
            "Epoch: 340/1000\n",
            "1.6583858087868846\n",
            "\n",
            "Epoch: 341/1000\n",
            "1.653165538728054\n",
            "\n",
            "Epoch: 342/1000\n",
            "1.647905272534011\n",
            "\n",
            "Epoch: 343/1000\n",
            "1.6510027923025499\n",
            "\n",
            "Epoch: 344/1000\n",
            "1.649300879285914\n",
            "\n",
            "Epoch: 345/1000\n",
            "1.6733649902748011\n",
            "\n",
            "Epoch: 346/1000\n",
            "1.646982829366388\n",
            "\n",
            "Epoch: 347/1000\n",
            "1.623960705406164\n",
            "\n",
            "Epoch: 348/1000\n",
            "1.6314687464187287\n",
            "\n",
            "Epoch: 349/1000\n",
            "1.6332101066929277\n",
            "\n",
            "Epoch: 350/1000\n",
            "1.6576999641754826\n",
            "\n",
            "Epoch: 351/1000\n",
            "1.629220190292542\n",
            "\n",
            "Epoch: 352/1000\n",
            "1.6347347106193004\n",
            "\n",
            "Epoch: 353/1000\n",
            "1.6089212913875477\n",
            "\n",
            "Epoch: 354/1000\n",
            "1.6114358409833986\n",
            "\n",
            "Epoch: 355/1000\n",
            "1.6178283959010804\n",
            "\n",
            "Epoch: 356/1000\n",
            "1.60170093068286\n",
            "\n",
            "Epoch: 357/1000\n",
            "1.5978558913686518\n",
            "\n",
            "Epoch: 358/1000\n",
            "1.60562781550492\n",
            "\n",
            "Epoch: 359/1000\n",
            "1.6304902970982513\n",
            "\n",
            "Epoch: 360/1000\n",
            "1.6137948732732699\n",
            "\n",
            "Epoch: 361/1000\n",
            "1.629825577806731\n",
            "\n",
            "Epoch: 362/1000\n",
            "1.62782987049572\n",
            "\n",
            "Epoch: 363/1000\n",
            "1.6000233226645455\n",
            "\n",
            "Epoch: 364/1000\n",
            "1.6105720641446175\n",
            "\n",
            "Epoch: 365/1000\n",
            "1.5945921355550488\n",
            "\n",
            "Epoch: 366/1000\n",
            "1.6311197170677558\n",
            "\n",
            "Epoch: 367/1000\n",
            "1.647453175788528\n",
            "\n",
            "Epoch: 368/1000\n",
            "1.6850277400387172\n",
            "\n",
            "Epoch: 369/1000\n",
            "1.6661378454661122\n",
            "\n",
            "Epoch: 370/1000\n",
            "1.6376725016714226\n",
            "\n",
            "Epoch: 371/1000\n",
            "1.603769355644079\n",
            "\n",
            "Epoch: 372/1000\n",
            "1.602277000229348\n",
            "\n",
            "Epoch: 373/1000\n",
            "1.6136301725741757\n",
            "\n",
            "Epoch: 374/1000\n",
            "1.6116398697122163\n",
            "\n",
            "Epoch: 375/1000\n",
            "1.6189436075145627\n",
            "\n",
            "Epoch: 376/1000\n",
            "1.6121139175919135\n",
            "\n",
            "Epoch: 377/1000\n",
            "1.6144967675377584\n",
            "\n",
            "Epoch: 378/1000\n",
            "1.606514599760669\n",
            "\n",
            "Epoch: 379/1000\n",
            "1.613819261949741\n",
            "\n",
            "Epoch: 380/1000\n",
            "1.6028758792922273\n",
            "\n",
            "Epoch: 381/1000\n",
            "1.6209123488823098\n",
            "\n",
            "Epoch: 382/1000\n",
            "1.6363284802279765\n",
            "\n",
            "Epoch: 383/1000\n",
            "1.628043164250665\n",
            "\n",
            "Epoch: 384/1000\n",
            "1.624592192478226\n",
            "\n",
            "Epoch: 385/1000\n",
            "1.5915118481977506\n",
            "\n",
            "Epoch: 386/1000\n",
            "1.5823872273962458\n",
            "\n",
            "Epoch: 387/1000\n",
            "1.5812160514333053\n",
            "\n",
            "Epoch: 388/1000\n",
            "1.5957259996451554\n",
            "\n",
            "Epoch: 389/1000\n",
            "1.6068624669429348\n",
            "\n",
            "Epoch: 390/1000\n",
            "1.6240683208319577\n",
            "\n",
            "Epoch: 391/1000\n",
            "1.5895402996983674\n",
            "\n",
            "Epoch: 392/1000\n",
            "1.5909695380533244\n",
            "\n",
            "Epoch: 393/1000\n",
            "1.5855558927530191\n",
            "\n",
            "Epoch: 394/1000\n",
            "1.5770895014402375\n",
            "\n",
            "Epoch: 395/1000\n",
            "1.5868531209739163\n",
            "\n",
            "Epoch: 396/1000\n",
            "1.6064714689076005\n",
            "\n",
            "Epoch: 397/1000\n",
            "1.6186805189303626\n",
            "\n",
            "Epoch: 398/1000\n",
            "1.631037486514035\n",
            "\n",
            "Epoch: 399/1000\n",
            "1.6050726365172556\n",
            "\n",
            "Epoch: 400/1000\n",
            "1.6159954821289813\n",
            "\n",
            "Epoch: 401/1000\n",
            "1.61269988979034\n",
            "\n",
            "Epoch: 402/1000\n",
            "1.6067751821200462\n",
            "\n",
            "Epoch: 403/1000\n",
            "1.619987075103528\n",
            "\n",
            "Epoch: 404/1000\n",
            "1.6156948897410919\n",
            "\n",
            "Epoch: 405/1000\n",
            "1.6261367443399217\n",
            "\n",
            "Epoch: 406/1000\n",
            "1.6126413564915265\n",
            "\n",
            "Epoch: 407/1000\n",
            "1.6069724260046172\n",
            "\n",
            "Epoch: 408/1000\n",
            "1.5818457462111124\n",
            "\n",
            "Epoch: 409/1000\n",
            "1.5877207862405922\n",
            "\n",
            "Epoch: 410/1000\n",
            "1.5890666571870233\n",
            "\n",
            "Epoch: 411/1000\n",
            "1.588970612022396\n",
            "\n",
            "Epoch: 412/1000\n",
            "1.593464175958785\n",
            "\n",
            "Epoch: 413/1000\n",
            "1.6001786077076021\n",
            "\n",
            "Epoch: 414/1000\n",
            "1.6005168059710733\n",
            "\n",
            "Epoch: 415/1000\n",
            "1.6072974441245347\n",
            "\n",
            "Epoch: 416/1000\n",
            "1.6046836646016844\n",
            "\n",
            "Epoch: 417/1000\n",
            "1.6105962955709676\n",
            "\n",
            "Epoch: 418/1000\n",
            "1.6066719120403616\n",
            "\n",
            "Epoch: 419/1000\n",
            "1.6155211717282243\n",
            "\n",
            "Epoch: 420/1000\n",
            "1.6385634391561943\n",
            "\n",
            "Epoch: 421/1000\n",
            "1.618214522166428\n",
            "\n",
            "Epoch: 422/1000\n",
            "1.611058796385708\n",
            "\n",
            "Epoch: 423/1000\n",
            "1.6182998078748776\n",
            "\n",
            "Epoch: 424/1000\n",
            "1.6098549158985256\n",
            "\n",
            "Epoch: 425/1000\n",
            "1.620023086569625\n",
            "\n",
            "Epoch: 426/1000\n",
            "1.6260635957160707\n",
            "\n",
            "Epoch: 427/1000\n",
            "1.6210838515481745\n",
            "\n",
            "Epoch: 428/1000\n",
            "1.6134225398593396\n",
            "\n",
            "Epoch: 429/1000\n",
            "1.646635721225852\n",
            "\n",
            "Epoch: 430/1000\n",
            "1.651742008231407\n",
            "\n",
            "Epoch: 431/1000\n",
            "1.6566547935074722\n",
            "\n",
            "Epoch: 432/1000\n",
            "1.6421200785458354\n",
            "\n",
            "Epoch: 433/1000\n",
            "1.6230514820352107\n",
            "\n",
            "Epoch: 434/1000\n",
            "1.6041797998719773\n",
            "\n",
            "Epoch: 435/1000\n",
            "1.6074373259716626\n",
            "\n",
            "Epoch: 436/1000\n",
            "1.6244946495364503\n",
            "\n",
            "Epoch: 437/1000\n",
            "1.6141319618406782\n",
            "\n",
            "Epoch: 438/1000\n",
            "1.6088036683160665\n",
            "\n",
            "Epoch: 439/1000\n",
            "1.63546285648438\n",
            "\n",
            "Epoch: 440/1000\n",
            "1.6382096069092795\n",
            "\n",
            "Epoch: 441/1000\n",
            "1.635525625828043\n",
            "\n",
            "Epoch: 442/1000\n",
            "1.6324932496003612\n",
            "\n",
            "Epoch: 443/1000\n",
            "1.6365754742498058\n",
            "\n",
            "Epoch: 444/1000\n",
            "1.6457703981247525\n",
            "\n",
            "Epoch: 445/1000\n",
            "1.6338590781929472\n",
            "\n",
            "Epoch: 446/1000\n",
            "1.6320303759885746\n",
            "\n",
            "Epoch: 447/1000\n",
            "1.6390393259199976\n",
            "\n",
            "Epoch: 448/1000\n",
            "1.6356287563325163\n",
            "\n",
            "Epoch: 449/1000\n",
            "1.6446153308846454\n",
            "\n",
            "Epoch: 450/1000\n",
            "1.6418718113949915\n",
            "\n",
            "Epoch: 451/1000\n",
            "1.6452875075240556\n",
            "\n",
            "Epoch: 452/1000\n",
            "1.656687496175464\n",
            "\n",
            "Epoch: 453/1000\n",
            "1.6326140093184096\n",
            "\n",
            "Epoch: 454/1000\n",
            "1.6314813555544696\n",
            "\n",
            "Epoch: 455/1000\n",
            "1.6213689179480588\n",
            "\n",
            "Epoch: 456/1000\n",
            "1.6313169760700441\n",
            "\n",
            "Epoch: 457/1000\n",
            "1.6093006949705648\n",
            "\n",
            "Epoch: 458/1000\n",
            "1.606332066670945\n",
            "\n",
            "Epoch: 459/1000\n",
            "1.6047452713820507\n",
            "\n",
            "Epoch: 460/1000\n",
            "1.6059833867040822\n",
            "\n",
            "Epoch: 461/1000\n",
            "1.6033254137967246\n",
            "\n",
            "Epoch: 462/1000\n",
            "1.6011865592172407\n",
            "\n",
            "Epoch: 463/1000\n",
            "1.603549437838315\n",
            "\n",
            "Epoch: 464/1000\n",
            "1.6064747384591533\n",
            "\n",
            "Epoch: 465/1000\n",
            "1.6116100305478565\n",
            "\n",
            "Epoch: 466/1000\n",
            "1.6300900263684381\n",
            "\n",
            "Epoch: 467/1000\n",
            "1.621441136161042\n",
            "\n",
            "Epoch: 468/1000\n",
            "1.6181340743051122\n",
            "\n",
            "Epoch: 469/1000\n",
            "1.6266479730896561\n",
            "\n",
            "Epoch: 470/1000\n",
            "1.612552280130866\n",
            "\n",
            "Epoch: 471/1000\n",
            "1.6170267377023828\n",
            "\n",
            "Epoch: 472/1000\n",
            "1.6091906278104542\n",
            "\n",
            "Epoch: 473/1000\n",
            "1.6286799574284558\n",
            "\n",
            "Epoch: 474/1000\n",
            "1.622193892448685\n",
            "\n",
            "Epoch: 475/1000\n",
            "1.6683787833094568\n",
            "\n",
            "Epoch: 476/1000\n",
            "1.672132982107464\n",
            "\n",
            "Epoch: 477/1000\n",
            "1.6636172596026533\n",
            "\n",
            "Epoch: 478/1000\n",
            "1.6696198201637933\n",
            "\n",
            "Epoch: 479/1000\n",
            "1.6964113512727774\n",
            "\n",
            "Epoch: 480/1000\n",
            "1.6960423338824528\n",
            "\n",
            "Epoch: 481/1000\n",
            "1.697025054948467\n",
            "\n",
            "Epoch: 482/1000\n",
            "1.6767714296615797\n",
            "\n",
            "Epoch: 483/1000\n",
            "1.6797010105199732\n",
            "\n",
            "Epoch: 484/1000\n",
            "1.6525324312333438\n",
            "\n",
            "Epoch: 485/1000\n",
            "1.6449507451632417\n",
            "\n",
            "Epoch: 486/1000\n",
            "1.6539849051290239\n",
            "\n",
            "Epoch: 487/1000\n",
            "1.6819156525502883\n",
            "\n",
            "Epoch: 488/1000\n",
            "1.6625082475533406\n",
            "\n",
            "Epoch: 489/1000\n",
            "1.6611134574763475\n",
            "\n",
            "Epoch: 490/1000\n",
            "1.6723483494042426\n",
            "\n",
            "Epoch: 491/1000\n",
            "1.6992278406034214\n",
            "\n",
            "Epoch: 492/1000\n",
            "1.7219620712187733\n",
            "\n",
            "Epoch: 493/1000\n",
            "1.716525409287307\n",
            "\n",
            "Epoch: 494/1000\n",
            "1.702812539009151\n",
            "\n",
            "Epoch: 495/1000\n",
            "1.6760576768042958\n",
            "\n",
            "Epoch: 496/1000\n",
            "1.6783844780373383\n",
            "\n",
            "Epoch: 497/1000\n",
            "1.6969512710617625\n",
            "\n",
            "Epoch: 498/1000\n",
            "1.6998510882615354\n",
            "\n",
            "Epoch: 499/1000\n",
            "1.6897261913723138\n",
            "\n",
            "Epoch: 500/1000\n",
            "1.6597488540670713\n",
            "\n",
            "Epoch: 501/1000\n",
            "1.6612960994476527\n",
            "\n",
            "Epoch: 502/1000\n",
            "1.6823219329431505\n",
            "\n",
            "Epoch: 503/1000\n",
            "1.7338663712028253\n",
            "\n",
            "Epoch: 504/1000\n",
            "1.7162389938613234\n",
            "\n",
            "Epoch: 505/1000\n",
            "1.699413099951752\n",
            "\n",
            "Epoch: 506/1000\n",
            "1.6839913899683827\n",
            "\n",
            "Epoch: 507/1000\n",
            "1.6599996037508236\n",
            "\n",
            "Epoch: 508/1000\n",
            "1.6728408549475458\n",
            "\n",
            "Epoch: 509/1000\n",
            "1.6572136851827097\n",
            "\n",
            "Epoch: 510/1000\n",
            "1.6737937651666832\n",
            "\n",
            "Epoch: 511/1000\n",
            "1.678536465798345\n",
            "\n",
            "Epoch: 512/1000\n",
            "1.6695225117905792\n",
            "\n",
            "Epoch: 513/1000\n",
            "1.6875967188307712\n",
            "\n",
            "Epoch: 514/1000\n",
            "1.681836204794759\n",
            "\n",
            "Epoch: 515/1000\n",
            "1.671132171161228\n",
            "\n",
            "Epoch: 516/1000\n",
            "1.6880494189511486\n",
            "\n",
            "Epoch: 517/1000\n",
            "1.7110907513160913\n",
            "\n",
            "Epoch: 518/1000\n",
            "1.7073576003342312\n",
            "\n",
            "Epoch: 519/1000\n",
            "1.705591358710934\n",
            "\n",
            "Epoch: 520/1000\n",
            "1.6743573471577522\n",
            "\n",
            "Epoch: 521/1000\n",
            "1.6726613867762519\n",
            "\n",
            "Epoch: 522/1000\n",
            "1.6631827526627205\n",
            "\n",
            "Epoch: 523/1000\n",
            "1.6681711171170355\n",
            "\n",
            "Epoch: 524/1000\n",
            "1.6740340966917173\n",
            "\n",
            "Epoch: 525/1000\n",
            "1.6726268627751304\n",
            "\n",
            "Epoch: 526/1000\n",
            "1.676481527281426\n",
            "\n",
            "Epoch: 527/1000\n",
            "1.6988162502955788\n",
            "\n",
            "Epoch: 528/1000\n",
            "1.692036383052473\n",
            "\n",
            "Epoch: 529/1000\n",
            "1.6803236470537894\n",
            "\n",
            "Epoch: 530/1000\n",
            "1.6947968210161641\n",
            "\n",
            "Epoch: 531/1000\n",
            "1.7062049694307146\n",
            "\n",
            "Epoch: 532/1000\n",
            "1.6911826057270587\n",
            "\n",
            "Epoch: 533/1000\n",
            "1.7021582248455087\n",
            "\n",
            "Epoch: 534/1000\n",
            "1.696056543805069\n",
            "\n",
            "Epoch: 535/1000\n",
            "1.7074492406986723\n",
            "\n",
            "Epoch: 536/1000\n",
            "1.6895971183530987\n",
            "\n",
            "Epoch: 537/1000\n",
            "1.6786383897394659\n",
            "\n",
            "Epoch: 538/1000\n",
            "1.689353151285502\n",
            "\n",
            "Epoch: 539/1000\n",
            "1.6831806477209914\n",
            "\n",
            "Epoch: 540/1000\n",
            "1.6800393991439293\n",
            "\n",
            "Epoch: 541/1000\n",
            "1.6915148772174466\n",
            "\n",
            "Epoch: 542/1000\n",
            "1.6932720354990491\n",
            "\n",
            "Epoch: 543/1000\n",
            "1.705607907429518\n",
            "\n",
            "Epoch: 544/1000\n",
            "1.716402955832744\n",
            "\n",
            "Epoch: 545/1000\n",
            "1.7008121920817287\n",
            "\n",
            "Epoch: 546/1000\n",
            "1.6800615680649187\n",
            "\n",
            "Epoch: 547/1000\n",
            "1.6846457125335832\n",
            "\n",
            "Epoch: 548/1000\n",
            "1.6746641007732936\n",
            "\n",
            "Epoch: 549/1000\n",
            "1.6736083744569612\n",
            "\n",
            "Epoch: 550/1000\n",
            "1.6835100983759532\n",
            "\n",
            "Epoch: 551/1000\n",
            "1.686260482778316\n",
            "\n",
            "Epoch: 552/1000\n",
            "1.678900655971211\n",
            "\n",
            "Epoch: 553/1000\n",
            "1.6943234989451106\n",
            "\n",
            "Epoch: 554/1000\n",
            "1.6781042988196635\n",
            "\n",
            "Epoch: 555/1000\n",
            "1.6912320063882234\n",
            "\n",
            "Epoch: 556/1000\n",
            "1.6867883062225582\n",
            "\n",
            "Epoch: 557/1000\n",
            "1.6685249527506192\n",
            "\n",
            "Epoch: 558/1000\n",
            "1.6777978750935403\n",
            "\n",
            "Epoch: 559/1000\n",
            "1.6683503034898501\n",
            "\n",
            "Epoch: 560/1000\n",
            "1.6819210697580664\n",
            "\n",
            "Epoch: 561/1000\n",
            "1.6842214241023064\n",
            "\n",
            "Epoch: 562/1000\n",
            "1.6887745904126856\n",
            "\n",
            "Epoch: 563/1000\n",
            "1.6897454542732233\n",
            "\n",
            "Epoch: 564/1000\n",
            "1.6768524553140713\n",
            "\n",
            "Epoch: 565/1000\n",
            "1.6847438447376037\n",
            "\n",
            "Epoch: 566/1000\n",
            "1.6832827453976265\n",
            "\n",
            "Epoch: 567/1000\n",
            "1.6766581543836705\n",
            "\n",
            "Epoch: 568/1000\n",
            "1.6971114788142103\n",
            "\n",
            "Epoch: 569/1000\n",
            "1.6899845174745678\n",
            "\n",
            "Epoch: 570/1000\n",
            "1.6953005577900788\n",
            "\n",
            "Epoch: 571/1000\n",
            "1.707526157165659\n",
            "\n",
            "Epoch: 572/1000\n",
            "1.7170476372383354\n",
            "\n",
            "Epoch: 573/1000\n",
            "1.7108010605178903\n",
            "\n",
            "Epoch: 574/1000\n",
            "1.7265036654497363\n",
            "\n",
            "Epoch: 575/1000\n",
            "1.7242725186637722\n",
            "\n",
            "Epoch: 576/1000\n",
            "1.6846944291720132\n",
            "\n",
            "Epoch: 577/1000\n",
            "1.6932219054243156\n",
            "\n",
            "Epoch: 578/1000\n",
            "1.6876209442253052\n",
            "\n",
            "Epoch: 579/1000\n",
            "1.7004657380747816\n",
            "\n",
            "Epoch: 580/1000\n",
            "1.6997858919551796\n",
            "\n",
            "Epoch: 581/1000\n",
            "1.709726925821839\n",
            "\n",
            "Epoch: 582/1000\n",
            "1.7191018719394784\n",
            "\n",
            "Epoch: 583/1000\n",
            "1.729097817306953\n",
            "\n",
            "Epoch: 584/1000\n",
            "1.727236204220406\n",
            "\n",
            "Epoch: 585/1000\n",
            "1.716662847921598\n",
            "\n",
            "Epoch: 586/1000\n",
            "1.717445566520392\n",
            "\n",
            "Epoch: 587/1000\n",
            "1.733028253159263\n",
            "\n",
            "Epoch: 588/1000\n",
            "1.7701914526587337\n",
            "\n",
            "Epoch: 589/1000\n",
            "1.734697029663375\n",
            "\n",
            "Epoch: 590/1000\n",
            "1.7384169471515585\n",
            "\n",
            "Epoch: 591/1000\n",
            "1.7317469350182408\n",
            "\n",
            "Epoch: 592/1000\n",
            "1.719125413443283\n",
            "\n",
            "Epoch: 593/1000\n",
            "1.7322286608115989\n",
            "\n",
            "Epoch: 594/1000\n",
            "1.7297936431613032\n",
            "\n",
            "Epoch: 595/1000\n",
            "1.750213784094817\n",
            "\n",
            "Epoch: 596/1000\n",
            "1.7448496468229566\n",
            "\n",
            "Epoch: 597/1000\n",
            "1.7167372953581839\n",
            "\n",
            "Epoch: 598/1000\n",
            "1.7295104770809706\n",
            "\n",
            "Epoch: 599/1000\n",
            "1.7278426732588428\n",
            "\n",
            "Epoch: 600/1000\n",
            "1.7295978742441185\n",
            "\n",
            "Epoch: 601/1000\n",
            "1.7185726524019214\n",
            "\n",
            "Epoch: 602/1000\n",
            "1.7355577381078615\n",
            "\n",
            "Epoch: 603/1000\n",
            "1.7318147451405204\n",
            "\n",
            "Epoch: 604/1000\n",
            "1.7305449644130197\n",
            "\n",
            "Epoch: 605/1000\n",
            "1.7292820302969387\n",
            "\n",
            "Epoch: 606/1000\n",
            "1.7342407600880343\n",
            "\n",
            "Epoch: 607/1000\n",
            "1.7379392384560193\n",
            "\n",
            "Epoch: 608/1000\n",
            "1.7257033876191392\n",
            "\n",
            "Epoch: 609/1000\n",
            "1.724336007977461\n",
            "\n",
            "Epoch: 610/1000\n",
            "1.7176134330194197\n",
            "\n",
            "Epoch: 611/1000\n",
            "1.7298839026085966\n",
            "\n",
            "Epoch: 612/1000\n",
            "1.743893598097269\n",
            "\n",
            "Epoch: 613/1000\n",
            "1.7516932119186401\n",
            "\n",
            "Epoch: 614/1000\n",
            "1.7634311704757666\n",
            "\n",
            "Epoch: 615/1000\n",
            "1.7491484686188092\n",
            "\n",
            "Epoch: 616/1000\n",
            "1.780562031697022\n",
            "\n",
            "Epoch: 617/1000\n",
            "1.7674547996144456\n",
            "\n",
            "Epoch: 618/1000\n",
            "1.7682277211924653\n",
            "\n",
            "Epoch: 619/1000\n",
            "1.7909799918164364\n",
            "\n",
            "Epoch: 620/1000\n",
            "1.7902526836516612\n",
            "\n",
            "Epoch: 621/1000\n",
            "1.7639312703172272\n",
            "\n",
            "Epoch: 622/1000\n",
            "1.7534971462504125\n",
            "\n",
            "Epoch: 623/1000\n",
            "1.7621563602558339\n",
            "\n",
            "Epoch: 624/1000\n",
            "1.7727199464458405\n",
            "\n",
            "Epoch: 625/1000\n",
            "1.743709658500873\n",
            "\n",
            "Epoch: 626/1000\n",
            "1.7340633216934511\n",
            "\n",
            "Epoch: 627/1000\n",
            "1.7369553809570828\n",
            "\n",
            "Epoch: 628/1000\n",
            "1.7254778335917131\n",
            "\n",
            "Epoch: 629/1000\n",
            "1.7288151323443843\n",
            "\n",
            "Epoch: 630/1000\n",
            "1.7287737900998668\n",
            "\n",
            "Epoch: 631/1000\n",
            "1.7250102354456862\n",
            "\n",
            "Epoch: 632/1000\n",
            "1.7564506351065083\n",
            "\n",
            "Epoch: 633/1000\n",
            "1.7733669693686145\n",
            "\n",
            "Epoch: 634/1000\n",
            "1.7813070927291654\n",
            "\n",
            "Epoch: 635/1000\n",
            "1.7715799954885298\n",
            "\n",
            "Epoch: 636/1000\n",
            "1.777941125794596\n",
            "\n",
            "Epoch: 637/1000\n",
            "1.7974732406407405\n",
            "\n",
            "Epoch: 638/1000\n",
            "1.8027156432260956\n",
            "\n",
            "Epoch: 639/1000\n",
            "1.7903072971863239\n",
            "\n",
            "Epoch: 640/1000\n",
            "1.776799245116425\n",
            "\n",
            "Epoch: 641/1000\n",
            "1.76625637531422\n",
            "\n",
            "Epoch: 642/1000\n",
            "1.7490145139932052\n",
            "\n",
            "Epoch: 643/1000\n",
            "1.7465794162106325\n",
            "\n",
            "Epoch: 644/1000\n",
            "1.7464713497771205\n",
            "\n",
            "Epoch: 645/1000\n",
            "1.7566230113760495\n",
            "\n",
            "Epoch: 646/1000\n",
            "1.7621555420137016\n",
            "\n",
            "Epoch: 647/1000\n",
            "1.7454348658550263\n",
            "\n",
            "Epoch: 648/1000\n",
            "1.7418539893869704\n",
            "\n",
            "Epoch: 649/1000\n",
            "1.7463443889356345\n",
            "\n",
            "Epoch: 650/1000\n",
            "1.754302671444134\n",
            "\n",
            "Epoch: 651/1000\n",
            "1.7743543126861032\n",
            "\n",
            "Epoch: 652/1000\n",
            "1.7663498207195414\n",
            "\n",
            "Epoch: 653/1000\n",
            "1.7725093268744816\n",
            "\n",
            "Epoch: 654/1000\n",
            "1.7670064066669615\n",
            "\n",
            "Epoch: 655/1000\n",
            "1.7728484171326764\n",
            "\n",
            "Epoch: 656/1000\n",
            "1.7881728136931259\n",
            "\n",
            "Epoch: 657/1000\n",
            "1.7699175836186547\n",
            "\n",
            "Epoch: 658/1000\n",
            "1.764311227145682\n",
            "\n",
            "Epoch: 659/1000\n",
            "1.7634042612390595\n",
            "\n",
            "Epoch: 660/1000\n",
            "1.7679001176305869\n",
            "\n",
            "Epoch: 661/1000\n",
            "1.7568054170455012\n",
            "\n",
            "Epoch: 662/1000\n",
            "1.7549558885809435\n",
            "\n",
            "Epoch: 663/1000\n",
            "1.7729652388819384\n",
            "\n",
            "Epoch: 664/1000\n",
            "1.7760275208111762\n",
            "\n",
            "Epoch: 665/1000\n",
            "1.766740097176433\n",
            "\n",
            "Epoch: 666/1000\n",
            "1.7584040622183172\n",
            "\n",
            "Epoch: 667/1000\n",
            "1.7569197342136846\n",
            "\n",
            "Epoch: 668/1000\n",
            "1.761258352795342\n",
            "\n",
            "Epoch: 669/1000\n",
            "1.762991454682421\n",
            "\n",
            "Epoch: 670/1000\n",
            "1.7626643170108958\n",
            "\n",
            "Epoch: 671/1000\n",
            "1.7686976913209902\n",
            "\n",
            "Epoch: 672/1000\n",
            "1.7589289360243558\n",
            "\n",
            "Epoch: 673/1000\n",
            "1.7542143499508343\n",
            "\n",
            "Epoch: 674/1000\n",
            "1.7629513963423014\n",
            "\n",
            "Epoch: 675/1000\n",
            "1.7656223562299151\n",
            "\n",
            "Epoch: 676/1000\n",
            "1.7694975705040767\n",
            "\n",
            "Epoch: 677/1000\n",
            "1.761103501450811\n",
            "\n",
            "Epoch: 678/1000\n",
            "1.7640708988192497\n",
            "\n",
            "Epoch: 679/1000\n",
            "1.7553638214846972\n",
            "\n",
            "Epoch: 680/1000\n",
            "1.7576386070405323\n",
            "\n",
            "Epoch: 681/1000\n",
            "1.7553894360681497\n",
            "\n",
            "Epoch: 682/1000\n",
            "1.7507268255007293\n",
            "\n",
            "Epoch: 683/1000\n",
            "1.7426592258419107\n",
            "\n",
            "Epoch: 684/1000\n",
            "1.758980074904185\n",
            "\n",
            "Epoch: 685/1000\n",
            "1.7661512199096736\n",
            "\n",
            "Epoch: 686/1000\n",
            "1.7604610329815749\n",
            "\n",
            "Epoch: 687/1000\n",
            "1.7543359000085925\n",
            "\n",
            "Epoch: 688/1000\n",
            "1.7610755278109045\n",
            "\n",
            "Epoch: 689/1000\n",
            "1.7689496761303922\n",
            "\n",
            "Epoch: 690/1000\n",
            "1.7704447448053928\n",
            "\n",
            "Epoch: 691/1000\n",
            "1.782322154320906\n",
            "\n",
            "Epoch: 692/1000\n",
            "1.7994770265615296\n",
            "\n",
            "Epoch: 693/1000\n",
            "1.7826474294328187\n",
            "\n",
            "Epoch: 694/1000\n",
            "1.7749427671303106\n",
            "\n",
            "Epoch: 695/1000\n",
            "1.7706247202712269\n",
            "\n",
            "Epoch: 696/1000\n",
            "1.7921133338121498\n",
            "\n",
            "Epoch: 697/1000\n",
            "1.7921145352903005\n",
            "\n",
            "Epoch: 698/1000\n",
            "1.788847313361991\n",
            "\n",
            "Epoch: 699/1000\n",
            "1.786516113342921\n",
            "\n",
            "Epoch: 700/1000\n",
            "1.7860909743649902\n",
            "\n",
            "Epoch: 701/1000\n",
            "1.7749976033957764\n",
            "\n",
            "Epoch: 702/1000\n",
            "1.7638471668772726\n",
            "\n",
            "Epoch: 703/1000\n",
            "1.7786903726293215\n",
            "\n",
            "Epoch: 704/1000\n",
            "1.7691221094742757\n",
            "\n",
            "Epoch: 705/1000\n",
            "1.7693144739081128\n",
            "\n",
            "Epoch: 706/1000\n",
            "1.7660218117218285\n",
            "\n",
            "Epoch: 707/1000\n",
            "1.7480524546621006\n",
            "\n",
            "Epoch: 708/1000\n",
            "1.7453240330860955\n",
            "\n",
            "Epoch: 709/1000\n",
            "1.7581941630170674\n",
            "\n",
            "Epoch: 710/1000\n",
            "1.7670574323635448\n",
            "\n",
            "Epoch: 711/1000\n",
            "1.766849655639287\n",
            "\n",
            "Epoch: 712/1000\n",
            "1.7678288010552348\n",
            "\n",
            "Epoch: 713/1000\n",
            "1.7988280395643157\n",
            "\n",
            "Epoch: 714/1000\n",
            "1.7945409879704008\n",
            "\n",
            "Epoch: 715/1000\n",
            "1.7948876090815202\n",
            "\n",
            "Epoch: 716/1000\n",
            "1.811914089529161\n",
            "\n",
            "Epoch: 717/1000\n",
            "1.8165980435215767\n",
            "\n",
            "Epoch: 718/1000\n",
            "1.8048335593550362\n",
            "\n",
            "Epoch: 719/1000\n",
            "1.8022177871666378\n",
            "\n",
            "Epoch: 720/1000\n",
            "1.7957609445558582\n",
            "\n",
            "Epoch: 721/1000\n",
            "1.7965516980333773\n",
            "\n",
            "Epoch: 722/1000\n",
            "1.7986657990338941\n",
            "\n",
            "Epoch: 723/1000\n",
            "1.7924727533358855\n",
            "\n",
            "Epoch: 724/1000\n",
            "1.7800924150033937\n",
            "\n",
            "Epoch: 725/1000\n",
            "1.7746778448121798\n",
            "\n",
            "Epoch: 726/1000\n",
            "1.7725884853911842\n",
            "\n",
            "Epoch: 727/1000\n",
            "1.7893337940360519\n",
            "\n",
            "Epoch: 728/1000\n",
            "1.7926915815058435\n",
            "\n",
            "Epoch: 729/1000\n",
            "1.7794480238030135\n",
            "\n",
            "Epoch: 730/1000\n",
            "1.772473583175897\n",
            "\n",
            "Epoch: 731/1000\n",
            "1.7617782537025268\n",
            "\n",
            "Epoch: 732/1000\n",
            "1.7737637952023926\n",
            "\n",
            "Epoch: 733/1000\n",
            "1.7728535149989133\n",
            "\n",
            "Epoch: 734/1000\n",
            "1.7677623369664\n",
            "\n",
            "Epoch: 735/1000\n",
            "1.7674499951020743\n",
            "\n",
            "Epoch: 736/1000\n",
            "1.796785934414762\n",
            "\n",
            "Epoch: 737/1000\n",
            "1.7979202685247577\n",
            "\n",
            "Epoch: 738/1000\n",
            "1.7833663881998427\n",
            "\n",
            "Epoch: 739/1000\n",
            "1.779501248781425\n",
            "\n",
            "Epoch: 740/1000\n",
            "1.779870993445413\n",
            "\n",
            "Epoch: 741/1000\n",
            "1.774329828936263\n",
            "\n",
            "Epoch: 742/1000\n",
            "1.7724122205813884\n",
            "\n",
            "Epoch: 743/1000\n",
            "1.783485219297391\n",
            "\n",
            "Epoch: 744/1000\n",
            "1.7740847919800606\n",
            "\n",
            "Epoch: 745/1000\n",
            "1.7817123364013454\n",
            "\n",
            "Epoch: 746/1000\n",
            "1.7864040914259403\n",
            "\n",
            "Epoch: 747/1000\n",
            "1.799122297920507\n",
            "\n",
            "Epoch: 748/1000\n",
            "1.787743426301201\n",
            "\n",
            "Epoch: 749/1000\n",
            "1.7818604041450132\n",
            "\n",
            "Epoch: 750/1000\n",
            "1.781536290007317\n",
            "\n",
            "Epoch: 751/1000\n",
            "1.7818870819925388\n",
            "\n",
            "Epoch: 752/1000\n",
            "1.8037511955829622\n",
            "\n",
            "Epoch: 753/1000\n",
            "1.8106696237870608\n",
            "\n",
            "Epoch: 754/1000\n",
            "1.7970864529258868\n",
            "\n",
            "Epoch: 755/1000\n",
            "1.7927685453110473\n",
            "\n",
            "Epoch: 756/1000\n",
            "1.7947739990285165\n",
            "\n",
            "Epoch: 757/1000\n",
            "1.7927269938520736\n",
            "\n",
            "Epoch: 758/1000\n",
            "1.789139289567828\n",
            "\n",
            "Epoch: 759/1000\n",
            "1.7909355426434552\n",
            "\n",
            "Epoch: 760/1000\n",
            "1.7962974986092208\n",
            "\n",
            "Epoch: 761/1000\n",
            "1.808407213579865\n",
            "\n",
            "Epoch: 762/1000\n",
            "1.819463387716297\n",
            "\n",
            "Epoch: 763/1000\n",
            "1.8051903882720841\n",
            "\n",
            "Epoch: 764/1000\n",
            "1.7916702877611754\n",
            "\n",
            "Epoch: 765/1000\n",
            "1.795627360173714\n",
            "\n",
            "Epoch: 766/1000\n",
            "1.7935984114048678\n",
            "\n",
            "Epoch: 767/1000\n",
            "1.8012300490247979\n",
            "\n",
            "Epoch: 768/1000\n",
            "1.798065454409989\n",
            "\n",
            "Epoch: 769/1000\n",
            "1.7991461690998092\n",
            "\n",
            "Epoch: 770/1000\n",
            "1.8044131321500805\n",
            "\n",
            "Epoch: 771/1000\n",
            "1.807006099445368\n",
            "\n",
            "Epoch: 772/1000\n",
            "1.8197596384950836\n",
            "\n",
            "Epoch: 773/1000\n",
            "1.8082873887211093\n",
            "\n",
            "Epoch: 774/1000\n",
            "1.8090112765470499\n",
            "\n",
            "Epoch: 775/1000\n",
            "1.8246134958585247\n",
            "\n",
            "Epoch: 776/1000\n",
            "1.83865663128124\n",
            "\n",
            "Epoch: 777/1000\n",
            "1.8514234581254587\n",
            "\n",
            "Epoch: 778/1000\n",
            "1.8460742572245255\n",
            "\n",
            "Epoch: 779/1000\n",
            "1.8208913553219526\n",
            "\n",
            "Epoch: 780/1000\n",
            "1.8201026092811423\n",
            "\n",
            "Epoch: 781/1000\n",
            "1.8271836743127539\n",
            "\n",
            "Epoch: 782/1000\n",
            "1.8291195015331738\n",
            "\n",
            "Epoch: 783/1000\n",
            "1.8225183145885229\n",
            "\n",
            "Epoch: 784/1000\n",
            "1.8251026586318317\n",
            "\n",
            "Epoch: 785/1000\n",
            "1.8241577555649844\n",
            "\n",
            "Epoch: 786/1000\n",
            "1.8139195330212972\n",
            "\n",
            "Epoch: 787/1000\n",
            "1.818571735854932\n",
            "\n",
            "Epoch: 788/1000\n",
            "1.81939520771444\n",
            "\n",
            "Epoch: 789/1000\n",
            "1.817474207044561\n",
            "\n",
            "Epoch: 790/1000\n",
            "1.8325738953549282\n",
            "\n",
            "Epoch: 791/1000\n",
            "1.8469925868069845\n",
            "\n",
            "Epoch: 792/1000\n",
            "1.8441302983016532\n",
            "\n",
            "Epoch: 793/1000\n",
            "1.8396744049393339\n",
            "\n",
            "Epoch: 794/1000\n",
            "1.8354532996357196\n",
            "\n",
            "Epoch: 795/1000\n",
            "1.8407283458322654\n",
            "\n",
            "Epoch: 796/1000\n",
            "1.8346437631339276\n",
            "\n",
            "Epoch: 797/1000\n",
            "1.8375019316302843\n",
            "\n",
            "Epoch: 798/1000\n",
            "1.8431668523253433\n",
            "\n",
            "Epoch: 799/1000\n",
            "1.8446006497579115\n",
            "\n",
            "Epoch: 800/1000\n",
            "1.8352849940992981\n",
            "\n",
            "Epoch: 801/1000\n",
            "1.8355279793398673\n",
            "\n",
            "Epoch: 802/1000\n",
            "1.8406800894741249\n",
            "\n",
            "Epoch: 803/1000\n",
            "1.842994120283672\n",
            "\n",
            "Epoch: 804/1000\n",
            "1.852396983771597\n",
            "\n",
            "Epoch: 805/1000\n",
            "1.8725290424105794\n",
            "\n",
            "Epoch: 806/1000\n",
            "1.8449693051939053\n",
            "\n",
            "Epoch: 807/1000\n",
            "1.8447958998709932\n",
            "\n",
            "Epoch: 808/1000\n",
            "1.8419135362671846\n",
            "\n",
            "Epoch: 809/1000\n",
            "1.8399719666064493\n",
            "\n",
            "Epoch: 810/1000\n",
            "1.8465909130889986\n",
            "\n",
            "Epoch: 811/1000\n",
            "1.8478616865372683\n",
            "\n",
            "Epoch: 812/1000\n",
            "1.8362610463412599\n",
            "\n",
            "Epoch: 813/1000\n",
            "1.8309921371633333\n",
            "\n",
            "Epoch: 814/1000\n",
            "1.8364606146073321\n",
            "\n",
            "Epoch: 815/1000\n",
            "1.864082806974779\n",
            "\n",
            "Epoch: 816/1000\n",
            "1.887312969385927\n",
            "\n",
            "Epoch: 817/1000\n",
            "1.8615675704941546\n",
            "\n",
            "Epoch: 818/1000\n",
            "1.8474567828243633\n",
            "\n",
            "Epoch: 819/1000\n",
            "1.8509828373737622\n",
            "\n",
            "Epoch: 820/1000\n",
            "1.85604293232315\n",
            "\n",
            "Epoch: 821/1000\n",
            "1.8572033356107658\n",
            "\n",
            "Epoch: 822/1000\n",
            "1.8739287007431518\n",
            "\n",
            "Epoch: 823/1000\n",
            "1.8653833255053802\n",
            "\n",
            "Epoch: 824/1000\n",
            "1.8625899042800362\n",
            "\n",
            "Epoch: 825/1000\n",
            "1.8754968416326505\n",
            "\n",
            "Epoch: 826/1000\n",
            "1.8606568009102051\n",
            "\n",
            "Epoch: 827/1000\n",
            "1.872128740476721\n",
            "\n",
            "Epoch: 828/1000\n",
            "1.8750881276856592\n",
            "\n",
            "Epoch: 829/1000\n",
            "1.8706613202294675\n",
            "\n",
            "Epoch: 830/1000\n",
            "1.8699950243595875\n",
            "\n",
            "Epoch: 831/1000\n",
            "1.862114885491774\n",
            "\n",
            "Epoch: 832/1000\n",
            "1.8515591225090708\n",
            "\n",
            "Epoch: 833/1000\n",
            "1.8504268112101678\n",
            "\n",
            "Epoch: 834/1000\n",
            "1.8575184750488782\n",
            "\n",
            "Epoch: 835/1000\n",
            "1.8580752908914504\n",
            "\n",
            "Epoch: 836/1000\n",
            "1.860649650508978\n",
            "\n",
            "Epoch: 837/1000\n",
            "1.8481381083954684\n",
            "\n",
            "Epoch: 838/1000\n",
            "1.833669031555739\n",
            "\n",
            "Epoch: 839/1000\n",
            "1.8522769424619296\n",
            "\n",
            "Epoch: 840/1000\n",
            "1.8556080268206256\n",
            "\n",
            "Epoch: 841/1000\n",
            "1.8433291490547727\n",
            "\n",
            "Epoch: 842/1000\n",
            "1.8581318800157638\n",
            "\n",
            "Epoch: 843/1000\n",
            "1.8581112548117331\n",
            "\n",
            "Epoch: 844/1000\n",
            "1.8506990295020467\n",
            "\n",
            "Epoch: 845/1000\n",
            "1.8636289600462175\n",
            "\n",
            "Epoch: 846/1000\n",
            "1.8669123827450564\n",
            "\n",
            "Epoch: 847/1000\n",
            "1.8697583169284275\n",
            "\n",
            "Epoch: 848/1000\n",
            "1.8745776715355462\n",
            "\n",
            "Epoch: 849/1000\n",
            "1.8729516140491083\n",
            "\n",
            "Epoch: 850/1000\n",
            "1.8658189915997456\n",
            "\n",
            "Epoch: 851/1000\n",
            "1.8949340165975648\n",
            "\n",
            "Epoch: 852/1000\n",
            "1.8717545395792752\n",
            "\n",
            "Epoch: 853/1000\n",
            "1.8786519159575048\n",
            "\n",
            "Epoch: 854/1000\n",
            "1.8725956393319316\n",
            "\n",
            "Epoch: 855/1000\n",
            "1.8880131384794634\n",
            "\n",
            "Epoch: 856/1000\n",
            "1.885769035158082\n",
            "\n",
            "Epoch: 857/1000\n",
            "1.877220619077041\n",
            "\n",
            "Epoch: 858/1000\n",
            "1.8682780244080683\n",
            "\n",
            "Epoch: 859/1000\n",
            "1.872988936676424\n",
            "\n",
            "Epoch: 860/1000\n",
            "1.8673765107581404\n",
            "\n",
            "Epoch: 861/1000\n",
            "1.8650519695452397\n",
            "\n",
            "Epoch: 862/1000\n",
            "1.8802610831665871\n",
            "\n",
            "Epoch: 863/1000\n",
            "1.8780491356339826\n",
            "\n",
            "Epoch: 864/1000\n",
            "1.8871225926238424\n",
            "\n",
            "Epoch: 865/1000\n",
            "1.8828707482031342\n",
            "\n",
            "Epoch: 866/1000\n",
            "1.8660691474909818\n",
            "\n",
            "Epoch: 867/1000\n",
            "1.8639808018295962\n",
            "\n",
            "Epoch: 868/1000\n",
            "1.8699796363436163\n",
            "\n",
            "Epoch: 869/1000\n",
            "1.8928162523157106\n",
            "\n",
            "Epoch: 870/1000\n",
            "1.883213912147398\n",
            "\n",
            "Epoch: 871/1000\n",
            "1.8761451430585168\n",
            "\n",
            "Epoch: 872/1000\n",
            "1.8924744189348073\n",
            "\n",
            "Epoch: 873/1000\n",
            "1.9130365366689217\n",
            "\n",
            "Epoch: 874/1000\n",
            "1.9024443918912448\n",
            "\n",
            "Epoch: 875/1000\n",
            "1.8856494166177449\n",
            "\n",
            "Epoch: 876/1000\n",
            "1.9091009043761007\n",
            "\n",
            "Epoch: 877/1000\n",
            "1.8821267533775576\n",
            "\n",
            "Epoch: 878/1000\n",
            "1.8801450999235823\n",
            "\n",
            "Epoch: 879/1000\n",
            "1.8736056165763306\n",
            "\n",
            "Epoch: 880/1000\n",
            "1.8894547457806867\n",
            "\n",
            "Epoch: 881/1000\n",
            "1.8882526052992699\n",
            "\n",
            "Epoch: 882/1000\n",
            "1.8895289227311194\n",
            "\n",
            "Epoch: 883/1000\n",
            "1.876031839358094\n",
            "\n",
            "Epoch: 884/1000\n",
            "1.8583436622814837\n",
            "\n",
            "Epoch: 885/1000\n",
            "1.8656271910649755\n",
            "\n",
            "Epoch: 886/1000\n",
            "1.8802151849664235\n",
            "\n",
            "Epoch: 887/1000\n",
            "1.8642260179281798\n",
            "\n",
            "Epoch: 888/1000\n",
            "1.879612229474969\n",
            "\n",
            "Epoch: 889/1000\n",
            "1.856016697114558\n",
            "\n",
            "Epoch: 890/1000\n",
            "1.8528972622381878\n",
            "\n",
            "Epoch: 891/1000\n",
            "1.882373064027012\n",
            "\n",
            "Epoch: 892/1000\n",
            "1.8972375468663818\n",
            "\n",
            "Epoch: 893/1000\n",
            "1.9006827903398351\n",
            "\n",
            "Epoch: 894/1000\n",
            "1.8929314626161409\n",
            "\n",
            "Epoch: 895/1000\n",
            "1.8959630751598784\n",
            "\n",
            "Epoch: 896/1000\n",
            "1.8919116795590356\n",
            "\n",
            "Epoch: 897/1000\n",
            "1.8936564646375476\n",
            "\n",
            "Epoch: 898/1000\n",
            "1.8762740007951226\n",
            "\n",
            "Epoch: 899/1000\n",
            "1.8656267332440561\n",
            "\n",
            "Epoch: 900/1000\n",
            "1.8566525331984474\n",
            "\n",
            "Epoch: 901/1000\n",
            "1.8672452179705992\n",
            "\n",
            "Epoch: 902/1000\n",
            "1.8621257416907389\n",
            "\n",
            "Epoch: 903/1000\n",
            "1.8618330277925796\n",
            "\n",
            "Epoch: 904/1000\n",
            "1.860301203570911\n",
            "\n",
            "Epoch: 905/1000\n",
            "1.8654347856523028\n",
            "\n",
            "Epoch: 906/1000\n",
            "1.870534554330408\n",
            "\n",
            "Epoch: 907/1000\n",
            "1.869213193575144\n",
            "\n",
            "Epoch: 908/1000\n",
            "1.8683485965316482\n",
            "\n",
            "Epoch: 909/1000\n",
            "1.8607782223200229\n",
            "\n",
            "Epoch: 910/1000\n",
            "1.851960920819301\n",
            "\n",
            "Epoch: 911/1000\n",
            "1.8644663789780946\n",
            "\n",
            "Epoch: 912/1000\n",
            "1.8724442284380962\n",
            "\n",
            "Epoch: 913/1000\n",
            "1.8761989129718293\n",
            "\n",
            "Epoch: 914/1000\n",
            "1.8781049034043746\n",
            "\n",
            "Epoch: 915/1000\n",
            "1.8650653989377002\n",
            "\n",
            "Epoch: 916/1000\n",
            "1.8717880104601587\n",
            "\n",
            "Epoch: 917/1000\n",
            "1.86772707029635\n",
            "\n",
            "Epoch: 918/1000\n",
            "1.8645419137637367\n",
            "\n",
            "Epoch: 919/1000\n",
            "1.8785381045445075\n",
            "\n",
            "Epoch: 920/1000\n",
            "1.8783807944589979\n",
            "\n",
            "Epoch: 921/1000\n",
            "1.8861393674959555\n",
            "\n",
            "Epoch: 922/1000\n",
            "1.8896870083471144\n",
            "\n",
            "Epoch: 923/1000\n",
            "1.8899610737578179\n",
            "\n",
            "Epoch: 924/1000\n",
            "1.88841290177773\n",
            "\n",
            "Epoch: 925/1000\n",
            "1.8760355580171606\n",
            "\n",
            "Epoch: 926/1000\n",
            "1.8858400082278697\n",
            "\n",
            "Epoch: 927/1000\n",
            "1.8947110344133977\n",
            "\n",
            "Epoch: 928/1000\n",
            "1.8964617762542533\n",
            "\n",
            "Epoch: 929/1000\n",
            "1.8912648330449102\n",
            "\n",
            "Epoch: 930/1000\n",
            "1.8872843743181662\n",
            "\n",
            "Epoch: 931/1000\n",
            "1.8940961104437712\n",
            "\n",
            "Epoch: 932/1000\n",
            "1.899799879083547\n",
            "\n",
            "Epoch: 933/1000\n",
            "1.9035202835165939\n",
            "\n",
            "Epoch: 934/1000\n",
            "1.8876770410164665\n",
            "\n",
            "Epoch: 935/1000\n",
            "1.89984685643461\n",
            "\n",
            "Epoch: 936/1000\n",
            "1.9001609788500218\n",
            "\n",
            "Epoch: 937/1000\n",
            "1.9023003471188595\n",
            "\n",
            "Epoch: 938/1000\n",
            "1.9025705872567702\n",
            "\n",
            "Epoch: 939/1000\n",
            "1.897820716807524\n",
            "\n",
            "Epoch: 940/1000\n",
            "1.9393925684997988\n",
            "\n",
            "Epoch: 941/1000\n",
            "1.9137801254109836\n",
            "\n",
            "Epoch: 942/1000\n",
            "1.901754253998229\n",
            "\n",
            "Epoch: 943/1000\n",
            "1.942301136927988\n",
            "\n",
            "Epoch: 944/1000\n",
            "1.990753558386869\n",
            "\n",
            "Epoch: 945/1000\n",
            "1.9719280183792773\n",
            "\n",
            "Epoch: 946/1000\n",
            "1.9925653058199344\n",
            "\n",
            "Epoch: 947/1000\n",
            "1.9750940681820595\n",
            "\n",
            "Epoch: 948/1000\n",
            "1.9937097249431333\n",
            "\n",
            "Epoch: 949/1000\n",
            "1.9942748314583125\n",
            "\n",
            "Epoch: 950/1000\n",
            "1.988269293671823\n",
            "\n",
            "Epoch: 951/1000\n",
            "2.015249837572442\n",
            "\n",
            "Epoch: 952/1000\n",
            "1.9889783945972446\n",
            "\n",
            "Epoch: 953/1000\n",
            "1.9714438894831623\n",
            "\n",
            "Epoch: 954/1000\n",
            "1.9714144640080575\n",
            "\n",
            "Epoch: 955/1000\n",
            "1.9612332554519787\n",
            "\n",
            "Epoch: 956/1000\n",
            "1.9621268119530992\n",
            "\n",
            "Epoch: 957/1000\n",
            "1.9456917943644008\n",
            "\n",
            "Epoch: 958/1000\n",
            "1.9364555995685098\n",
            "\n",
            "Epoch: 959/1000\n",
            "1.95035079590818\n",
            "\n",
            "Epoch: 960/1000\n",
            "1.968160098769286\n",
            "\n",
            "Epoch: 961/1000\n",
            "1.968183591620774\n",
            "\n",
            "Epoch: 962/1000\n",
            "1.9881301126306168\n",
            "\n",
            "Epoch: 963/1000\n",
            "2.0016619287632924\n",
            "\n",
            "Epoch: 964/1000\n",
            "2.0068451522591855\n",
            "\n",
            "Epoch: 965/1000\n",
            "1.993736018573421\n",
            "\n",
            "Epoch: 966/1000\n",
            "1.9821050899318575\n",
            "\n",
            "Epoch: 967/1000\n",
            "1.9779154292090069\n",
            "\n",
            "Epoch: 968/1000\n",
            "2.0022398732496085\n",
            "\n",
            "Epoch: 969/1000\n",
            "2.016303662088463\n",
            "\n",
            "Epoch: 970/1000\n",
            "1.9945218171283863\n",
            "\n",
            "Epoch: 971/1000\n",
            "2.015446067437783\n",
            "\n",
            "Epoch: 972/1000\n",
            "1.988950840902366\n",
            "\n",
            "Epoch: 973/1000\n",
            "1.9781715616018913\n",
            "\n",
            "Epoch: 974/1000\n",
            "2.005595237426231\n",
            "\n",
            "Epoch: 975/1000\n",
            "2.0188296787867968\n",
            "\n",
            "Epoch: 976/1000\n",
            "2.0281171851333193\n",
            "\n",
            "Epoch: 977/1000\n",
            "2.0368818976161465\n",
            "\n",
            "Epoch: 978/1000\n",
            "2.0354455932666413\n",
            "\n",
            "Epoch: 979/1000\n",
            "2.0378893057393666\n",
            "\n",
            "Epoch: 980/1000\n",
            "2.0612589577036293\n",
            "\n",
            "Epoch: 981/1000\n",
            "2.0274689492108067\n",
            "\n",
            "Epoch: 982/1000\n",
            "2.002889785853916\n",
            "\n",
            "Epoch: 983/1000\n",
            "2.008966764821924\n",
            "\n",
            "Epoch: 984/1000\n",
            "2.0354227976959844\n",
            "\n",
            "Epoch: 985/1000\n",
            "2.031950720832281\n",
            "\n",
            "Epoch: 986/1000\n",
            "2.042091016513555\n",
            "\n",
            "Epoch: 987/1000\n",
            "2.0150640617722315\n",
            "\n",
            "Epoch: 988/1000\n",
            "1.9966996756667046\n",
            "\n",
            "Epoch: 989/1000\n",
            "1.9906051943111607\n",
            "\n",
            "Epoch: 990/1000\n",
            "1.977570271162508\n",
            "\n",
            "Epoch: 991/1000\n",
            "1.9947540017652252\n",
            "\n",
            "Epoch: 992/1000\n",
            "1.985271483693573\n",
            "\n",
            "Epoch: 993/1000\n",
            "1.9616481677066602\n",
            "\n",
            "Epoch: 994/1000\n",
            "1.965495690321365\n",
            "\n",
            "Epoch: 995/1000\n",
            "1.954801352681599\n",
            "\n",
            "Epoch: 996/1000\n",
            "1.959545592696678\n",
            "\n",
            "Epoch: 997/1000\n",
            "1.9756735890760473\n",
            "\n",
            "Epoch: 998/1000\n",
            "1.9953026239802147\n",
            "\n",
            "Epoch: 999/1000\n",
            "1.960497864649777\n",
            "\n",
            "Epoch: 1000/1000\n",
            "1.972698043476462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HAMduj7iHimz",
        "colab_type": "code",
        "outputId": "9d5ea9c3-4c59-405b-ab86-4071ca935f59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the training error for every iteration\n",
        "# in every epoch\n",
        "\n",
        "# TODO Implement\n",
        "plt.plot(nn_params.batch_cost_list)\n",
        "plt.xlabel(\"n_batch\")\n",
        "plt.ylabel(\"training_error\")\n",
        "plt.title(\"Training error in every iteration in every epoch\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAGACAYAAAC6OPj9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fe74c194160>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "RF-CeJJPHim6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Points:** $0.0$ of $1.0$\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "fsYLm8cwHim7",
        "colab_type": "code",
        "outputId": "45b83d50-9fc7-40ea-cd54-00825f0903ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "cell_type": "code",
      "source": [
        "# Plot the training error in every epoch\n",
        "# TODO Implement\n",
        "plt.plot(nn_params.cost_)\n",
        "plt.xlabel(\"n_epochs\")\n",
        "plt.ylabel(\"training_error\")\n",
        "plt.title(\"Training error in every epoch\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAGACAYAAAC6OPj9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fe74c2156a0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "vtbsFyHnHinB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Points:** $0.0$ of $1.0$\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "puSxT8rxHinC",
        "colab_type": "code",
        "outputId": "556ccd76-013b-4af3-dcff-c23a43a8b0b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "# Compute Training Accuracy\n",
        "# TODO Implement\n",
        "acc_training = nn.evaluation(X_trainval, Y_trainval)\n",
        "\n",
        "\n",
        "print('Training accuracy: %.2f%%' % (acc_training * 100))\n",
        "\n",
        "\n",
        "# Compute Test Accuracy\n",
        "# TODO Implement\n",
        "acc_testing = nn.evaluation(X_test, Y_test)\n",
        "print('Test accuracy: %.2f%%' % (acc_testing * 100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:74: RuntimeWarning: overflow encountered in exp\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training accuracy: 77.82%\n",
            "Test accuracy: 76.53%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S2YoKNM4HinG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Points:** $0.0$ of $0.5$\n",
        "**Comments:** None\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "rpXeKU1CHinJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Submission instructions\n",
        "You should provide a single Jupyter notebook (.ipynb file) as the solution. Put the names and student ids of your team members below. **Make sure to submit only 1 solution to only 1 tutor.**\n",
        "\n",
        "- Jane Doe, 123456\n",
        "- Jane Doe, 123456\n",
        "- Jane Doe, 123456"
      ]
    },
    {
      "metadata": {
        "id": "i1CpZYkHHinK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Points: 0.0 of 30.0 points"
      ]
    }
  ]
}